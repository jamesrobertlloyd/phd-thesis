\input{common/header.tex}
\inbpdocument

\chapter{Application of compositional kernels to a data mining competition}
\label{ch:gefcom}

In this chapter I will discuss how I used Gaussian processes with compositionally constructed kernels as part of an ensemble of predictors in a data mining competition.
In particular, this competition involved forecasting hourly loads of a US energy utility as part of the load forecasting track of the Global Energy Forecasting Competition 2012 hosted on Kaggle.
The chapter begins with a description of the relatively simple techniques I used to be ranked \nth{2} in the competition; this description is based on~\ref{Lloyd2013-yn}\fTBD{I do not know why this reference is not working at the moment}.
This work was completed before the work described in chapter~\ref{ch:construction} and hence the construction of kernels was not automated.
The chapter therefore concludes with an investigation of whether the kernels I used could have been automatically constructed.
This investigation identifies some short comings in the literature on approximation inference methods for Gaussian processes and I hypothesise about possible extensions to approximate inference methods that would overcome these difficulties.

\section{Introduction}

\subsection{Competition and data}

The data for this competition consisted of an hourly time series of energy utility loads at 20 zones with unknown location and temperatures at 13 stations also with unknown location.
The data stretched over a period of 4.5 years and 8 non-consecutive weeks of load data were missing.
The task of the competition was to fill in the missing 8 weeks (backcasting) and to predict loads for the week following the extent of the data where temperature data was not available (forecasting); use of additional sources of temperature data was expressly prohibited by the competition organisers.

The evaluation metric of the competition (weighted root mean squared error) was heavily biased towards good performance on the forecasting task.
Whilst the competition was running, contestants could submit predictions to the website Kaggle and obtain the value of the evaluation metric evaluated on 25\% of the held out data.
This value was known as the public score since it was published during the competition.
Perhaps not too surprisingly this resulted in many participants overfitting heavily to this particular split of the data; the leaderboard changed substantially when final scores were evaluated on the remaining 75\% of the data (yielding the private scores).
Further details can be found in the competition summary paper \citep{Hong2014-yf}.

\subsection{Structure of chapter}

This chapter is split into sections describing each technique used for forecasting temperatures and loads.
Within each section I have motivated the particular choice of algorithm, discussed how it was used and described how to replicate the results using the spreadsheets and scripts available at \url{https://github.com/jamesrobertlloyd/GEFCOM2012}.
The source code is not necessary for understanding this chapter, it is referenced purely for those interested in replicating my work (submissions to the competition were required to be entirely replicable).
The chapter then concludes with attempts to apply the kernel construction procedure of chapter~\ref{ch:construction} in order to automatically build the kernels I used in this competition.

\subsection{Methodology}

I approached the competition in the spirit of a data mining competition.
Consequently, some of the choices detailed below were based on intuition to save time and focus on the aspects of the data most likely to give the largest increases in performance.
Many of these choices could have been replaced by appropriate searches and cross validation; where more complex techniques would be required I have briefly described how one could perform a more objective analysis.
In general this chapter is much less rigorous then the rest of this thesis but it is hoped it provides an interesting example of the practical uses of the techniques described in chapter~\ref{ch:construction} and also highlights how data analysis actually proceeds when there are strict time deadlines and the only measure of success is predictive performance.

My methodology for load back / forecasting was to try different general purpose regression algorithms and then ensemble (average) the predictions.
The final ensemble comprised predictions from a gradient boosting machine (GBM), Gaussian process (GP) regression and the benchmark solution provided by the competition organisers (a linear model).

\section{Data cleansing}

\label{sec:cleans}

A sensible first step in any prediction task is to look at your data; in particular searching for anomalies.
This was performed for both temperature and load data using the spreadsheets \texttt{temp/temp.xlsx} and \texttt{load/load.xlsx} by plotting the data as time series and using various types of conditional formatting to search for irregularities visually.

\begin{figure}[ht]
  \begin{center}
    \input{figures/gefcom/load.tex}
  \end{center}
  \caption{Left: Raw data at zone 10, showing a large discontinuity. Right: Raw data at zone 9, showing atypical load dynamics.}
  \label{fig:load}
\end{figure}

This visual analysis revealed a large discontinuity in load series 10 and atypical dynamics in series 9 (see figure~\ref{fig:load} and others for comparison).
No other large anomalies were detected during the first inspection of the data and smaller irregularities were not revisited since the algorithms detailed below performed acceptably well without further data cleansing.

No adjustments were made for holidays or other irregular events such as electricity black-outs.
Ignoring them was merely a practicality of time constraints rather than a design choice.

\section{Temperature forecasting}

\label{sec:temp}

\subsection{Initial analysis and remarks}

The error metric used in GEFCom2012 was heavily weighted towards times at which temperatures were unknown (\ie the load forecast rather than backcasts).
Consequently, good temperature predictions seemed crucial for overall success.

When submitting a solution to Kaggle, the error metric was computed on 25\% of the held out data and returned to the user.
This allowed a user to optimise temperature predictions by optimising the score of resulting load predicitions (\ie computing load predictions based on different temperature predictions and selecting the temperature prediction with the highest corresponding load prediction score).
Depending on how the test data was split (into the 25\% test and 75\% validation partition) this may have allowed users to come very close to knowing the true future temperatures.

\begin{figure}[ht]
  \begin{center}
    \input{figures/gefcom/temp_pred.tex}
  \end{center}
  \caption{Raw data at temperature station 1 (after removing mean and scaling to have unit standard deviation) (black solid). Historic average of smoothed temperatures (blue dashed thick). Current and predicted smoothed temperatures (green dotted thick) and final temperature predictions (red dashed).}
  \label{fig:temp_pred}
\end{figure}

I therefore used a flexible but simple method for forecasting temperatures that could be easily tuned.
Figure~\ref{fig:temp_pred} shows data from temperature station 1 along with various curves used for prediction (described later).
The black solid line is the raw data, which shows that temperatures follow a smooth trend with a daily pattern of rising and falling temperatures.
For simplicity, I modelled the smooth trend and daily periodicity separately and modelled each temperature station in isolation.

\subsection{Methodology}

The smooth trend was estimated within the data using local linear regression \citep[e.g. chapter 6 of][]{Hastie2009-hj} with a bandwidth of one day; see section~\ref{sec:gefcom:auto} for a discussion of how this method could have been usefully replaced by Gaussian process regression.
I assumed that the smooth temperature trend would, on average, smoothly return to the historical average.
A suitable modelling technique that would give this type of prediction is Gaussian process regression; the difference between the smoothed temperature and its historical average was regressed against time using a kernel of the form $\kSE + \kWN$ \ie a smooth function plus noise.

In Figure~\ref{fig:temp_pred}, the thick blue dashed curve shows the historical average of smoothed temperatures and the thick green dotted line shows the current smoothed temperature and prediction.
The parameters of the GP model (length scale and scale factor of the squared exponential kernel) were tuned by hand, initially choosing sensible values based on plots of the data and then trying to optimise the public test score of derived load predictions.

The difference between the temperature and the smoothed temperature was assumed to be a smooth periodic function with a period of one day.
This modelling assumption was implemented using a Gaussian process with a periodic kernel (again regressing the difference against time).
Parameters of the GP model were chosen by optimising the marginal likelihood of the model given particular parameters using optimisation methods supplied in the GPML toolbox\footnotemark.
Only the last 250 data points (which corresponds to approximately 10 days) were used to fit this part of the model in order to capture recent temperature dynamics.
\footnotetext{\url{http://www.gaussianprocess.org/gpml/code/matlab/doc/}}

\subsection{Replication of results}

The spreadsheet and scripts used to manipulate the temperature data can be found in the folder \texttt{temp}.
The MATLAB script makes use of the GPML toolbox which is contained in the folder \texttt{GPML}.
The temperature data was reshaped into a (time) $\times$ (temperature station) array using the spreadsheet \texttt{temp.xlsx} and then saved in \texttt{times\_series\_raw.csv}.
The method described above is implemented by the MATLAB script \texttt{predict\_temp.m} which saves the predicted time series in \texttt{GP\_pred\_temp.csv} and the smoothing of these predictions in \texttt{smooth\_temp\_GP.mat} and \texttt{smooth\_temp\_GP.csv}.

\subsection{Discussion}

The red curve in Figure~\ref{fig:temp_pred} shows the result of the methodology applied to temperature station 1.
The fit to the data is far from perfect but it has captured the broad features of the data.
Given the time constraints of this competition I did not investigate the temperature / weather forecasting literature; far better methods must certainly exist!

\section{Gradient boosting machines}

\label{sec:gbm}

\subsection{Initial analysis and remarks}

\label{sec:gbm_init_anal}

\begin{figure}[ht]
  \begin{center}
    \input{figures/gefcom/load_temp.tex}
  \end{center}
  \caption{Raw data at load zone 1 (after removing mean and scaling to have unit standard deviation) (blue solid) and raw data at temperature station 9 (after removing mean and scaling to have unit standard deviation) (green dashed).}
  \label{fig:load_temp}
\end{figure}

Figure~\ref{fig:load_temp} plots a subset of the data for loads at zone 1 and temperature station 9.
We can see that that the load has a smoothly varying trend with roughly periodic daily deviations from this trend.
The load also appears to be negatively correlated with temperature in this region of data.
It is also reasonable to assume that load dynamics will be different on weekends compared to weekdays.

Based on these observations, I modelled loads as a function of time of day, time within week, temperatures and smoothed temperatures (all stations).
All zones were modelled independently for simplicity.

\subsection{Methodology}

A standard `black-box' technique for learning an unknown regression function is that of using gradient boosting machines (GBM) \citep[e.g. chapter 10 of][]{Hastie2009-hj}.
For each zone I used all of the data to train a GBM and then used that to predict all outputs \ie I learnt a global function for each zone.

I used a standard R implementation of GBM using most of the default settings\footnotemark.
After some brief experimentation with parameter values I used a shrinkage factor of 0.01, 10000 trees and an interaction depth of 3.
I experimented with other parameter values, using the public test score as a guide, but nothing I tried improved upon my initial guess and therefore I did not expend much effort trying to optimise parameter values.

\footnotetext{\url{http://cran.r-project.org/web/packages/gbm/index.html}}

\subsection{Replication of results}

The scripts and data used can be found in the folder \texttt{gbm}.
The load data was reshaped into a (time) $\times$ (load zone) array using the spreadsheet \texttt{load.xlsx} in the folder \texttt{load}.
This was then concatenated with the temperature predictions and smoothed temperature predictions and saved in \texttt{gbm\_input.csv}.
The R script \texttt{basic\_gbm.R} then performs the method described above to produce predictions which are saved in \texttt{gbm\_output.csv}.

\subsection{Discussion}

The time series for zone 10 had a very large discontinuity (see left of figure~\ref{fig:load}).
GBM was used as a global prediction method which means it will have produced very bad predictions for zone 10.
I later fixed this by modelling the loads either side of the discontinuity separately.
Perversely, this resulted in a worse public test score.
However, this change resulted in a much improved private test score; in hindsight I should have opted for the more reasonable model!

\section{Gaussian processes}

\label{sec:gp}

\subsection{Initial analysis and remarks}

The observations made about the load data in section~\ref{sec:gbm_init_anal} (\ie smooth trends, near periodic variations, dependence on temperature) can be modelled using Gaussian process regression.
As demonstrated in chapter~\ref{ch:construction} specifying the kernel function of a Gaussian process allows one to encode these structural assumptions into a regression model.

\subsection{Methodology}

Three different kernel functions were used; one for backcasting, one for forecasting and one for backcasting zone 9.
The forms of the three kernels used were as follows\footnotemark
\footnotetext{See source code for parameter values}
\[
& \kSE_t + \kSE_S + \kSE_t \times \kSE_T \times \kPer_t \label{eqn:kernel1}\\
& \kSE_t + \kSE_t + \kPer_t \label{eqn:kernel2}\\
& \kSE_t + \kSE_S + \kSE_T \times \kPer_t \label{eqn:kernel3}
\]
where the subscripts indicate which dimension the kernel acts upon; (t)ime, (T)emperature or (S)moothed temperature.
For backcasting, the model was applied to the 1000 surrounding data points; for forecasting the last 500 data points were used.
Only one temperature station was used in this method; for each back / forecasting region the model evidence of the model was computed for each temperature station.
Bayesian model averaging \citep[e.g.][]{Hoeting1999-tn} was then used to combine the predictions.
In practice this was usually numerically equivalent to selecting the model / temperature station with the highest model evidence.

Kernel~\eqref{eqn:kernel1} encodes the assumptions that loads can be explained as a smoothly varying trend and an approximately periodic component.
The smooth trend is the sum of a smooth function of smoothed temperature and a smooth function of time (to explain any deviations from the trend implied by temperature).
The periodic component can vary through time and by being multiplied by $\SE_t$ and $\SE_T$ the shape of the periodicity will be more similar to recent times and any points in time with similar temperatures.
This kernel was used for backcasting all zones apart from zone 9.
An example of this kernel in action is shown in Figure~\ref{fig:load_pred}.
The left plot shows the load data (black solid) and the prediction (red dashed).
On the right is the temperature time series which resulted in the model with the highest model evidence.
Comparing the two plots we can see that the deviations of the prediction from the trend through time correspond to fluctuations in the temperature time series.

\begin{figure}[ht]
  \begin{center}
    \input{figures/gefcom/load_pred.tex}
  \end{center}
  \caption{Left: Raw data (after scaling and translation) (black solid) and Gaussian process predictions (red dashed) for zone 1. Right: Corresponding temperature time series used for prediction. Notice that the peak in temperature corresponds to a trough in the load predictions \ie the model has predicted a negative correlation between temperatures and loads.}
  \label{fig:load_pred}
\end{figure}

Kernel~\eqref{eqn:kernel2} encodes the assumption that loads can be explained as the sum of two smooth functions with different length scales and an exactly periodic smooth function.
This simpler kernel was used for forecasting since it gave more stable extrapolations of the data.

Kernel~\eqref{eqn:kernel3} is a simpler version of kernel~\eqref{eqn:kernel1} used for backcasting zone 9.
The periodic component is now only similar to other times with similar temperatures.
This prevented the predictions from being so greatly affected by the large isolated load fluctuations present in zone 9.

The forms and parameters of the kernel functions (see the accompanying code for all of the values) were chosen by trial and error (observing which kernels gave reasonable looking predictions and computing public test scores).

\subsection{Replication of results}

The spreadsheet and scripts used can be found in the folder \texttt{gp}.
The load data was reshaped into a (time) $\times$ (load zone) array using the spreadsheet \texttt{load.xlsx} in the folder \texttt{load} and then saved in \texttt{load\_input.csv} which encodes missing values as zeros.
The outputs of the temperature prediction \texttt{GP\_pred\_temp.csv} and \texttt{smooth\_temp\_GP.mat} are used directly.

The MATLAB script \texttt{gp\_elec.m} performs the methodology described above, saving the predictions to \texttt{gp\_pred.csv}.

\subsection{Discussion}

Selecting the form and parameters of the kernels by hand required a moderate amount of familiarity with Gaussian processes.
It is likely that better performance could have been achieved by the larger and more principled search over kernel functions introduced in chapter~\ref{ch:construction}.
Barriers to its immediate application are however discussed in section~\ref{sec:gefcom:auto}.

\section{Selecting the final ensemble}

\label{sec:ensemble}

\subsection{Remarks}

The final prediction was formed as the ensemble (weighted average) of predictions from three separate methods; gradient boosting machines, Gaussian process regression and the benchmark solution provided by the competition organisers (a linear model).
Ensembling is a very powerful technique for combining predictions from separate models.
If two predictions (viewed as random quantities) are statistically uncorrelated then their average will have lower variance than either one separately.

In practice, different algorithms are correlated and ensembling highly correlated predictions will rarely lead to improved performance.
In particular, I also tried the random forest algorithm\fTBD{ref} but the predictions were too correlated with gradient boosting machines (they are both based on an ensemble of decision trees) to be a useful addition to the ensemble.

\subsection{Methodology}

The ensemble weights were chosen by hand, using the public test score as the metric to be optimised.
The public test scores did not vary considerably once sensible parameters had been found so more advanced techniques were not used to optimise the ensemble weights.

\subsection{Replication of results}

The spreadsheets used to manipulate model output can be found in the folder \texttt{ensemble}.
The spreadsheet \texttt{sub\_creator.xlsx} was used to convert (time) $\times$ (load zone) arrays of model output into the required format for submission.
The output of this spreadsheet was then copied in \texttt{ensemble\_creator.xlsx} which performs the averaging to produce ensembles of the predictions.

\subsection{Discussion}

This method could have been been slightly improved by programmatically selecting the ensemble weights, optimising some form of validation metric.
One could also have tried to optimise the public test score directly.
Since only 2 submissions could be tested per day, one would want to use an optimisation technique that made very efficient use of data \citep[e.g.][]{Osborne2009-ti, Snoek2012-ri}.

Larger improvements could likely be achieved by considering a larger number of base predictions.
It is likely that the algorithms used by other competitors in GEFCom2012 would provide useful additions to the ensemble of predictions.
See \cite{Hong2014-yf} and the references therein for an overview of the methods used by other participants.

\section{An attempt to construct the composite kernels automatically}
\label{sec:gefcom:auto}

My participation in this data mining competition provided excellent experience before working on the research described in chapters~\ref{ch:construction}~and~\ref{ch:description}.
A natural question to ask is whether or not the compositional kernel search could have been directly applied to this competition, potentially speeding up my development of models and improving performance.
The answer to this question is `mostly' and reveals some interesting shortcomings in methods of approximate inference for Gaussian processes.
We now present a detailed analysis of the temperature data.
These comments apply similarly to the load data.

The function used to forecast temperatures displayed in figure~\ref{fig:temp_pred} can be described as
\[
  & \textrm{ Average historical smoothed temperatures} \\
  + & \textrm{ A smooth function with a lengthscale of a few days } \\
  + & \textrm{ A daily periodic function}
\]
which could alternatively be described as
\[
  & \textrm{ An annual periodic function} \\
  + & \textrm{ A smooth function with a lengthscale of a few days } \\
  + & \textrm{ A daily periodic function}
\]
which could be modelled by a Gaussian process with kernel $\kPer + \kSE + \kPer$ with appropriate parameters.

Exact inference for Gaussian processes has $\mathcal{O}(N^3)$  computational complexity where $N$ is the number of data points.
This makes it (currently) prohibitive to perform exact inference for $N$ much larger than 1000.
The complete GEFCom dataset contained roughly 40,000 data points meaning that one has to consider approximate inference schemes.

A simple approximate inference scheme is to use a subset of the data to perform inference\citep[e.g.][]{Quinonero-Candela2005-er}.
Running the kernel search procedure on a randomly selected 500 data point subset of the data for temperature station 1 results in a model composed of the following additive components:
\begin{itemize}
  \item A very smooth function
  \item A periodic function with a period of 1.0 years
  \item An approximately periodic function with a period of 24.0 hours
  \item Uncorrelated noise
\end{itemize}
This model has captured the long term trend of the data and the two forms of periodicity but it has failed to identify that short term (over several days) temperature fluctuations are temporally correlated (this is due to weather effects).
This will result in short term extrapolations that are potentially separated by a discontinuity from the data\fTBD{It would be nice to plot this}.

Presented with this failure one might consider instead using a contiguous subset closest to the times at which extrapolation will be performed.
The results of running the kernel search on this data subset is a model composed of
\begin{itemize}
  \item A constant
  \item A periodic function with a period of 24.0 days
  \item A smooth function with a lengthscale of 16.5 hours
  \item A rapidly varying smooth function with a lengthscale of 2.0 hours
  \item Uncorrelated noise
\end{itemize}
which is visualised in figure~\ref{fig:gefcom:contig}.
This model has successfully captured the correlation in the data at a scale of days but since it only had access to about 20 days of data it is completely unaware of the annual periodicity / historical average temperatures.
Not including the annual periodicity may result in poor extrapolations when looking more than a few days ahead.

\begin{figure}[ht]
\centering
\includegraphics[width=0.23\columnwidth]{\gefcomfigsdir/station1contigsubset_1}
\includegraphics[width=0.23\columnwidth]{\gefcomfigsdir/station1contigsubset_2}
\includegraphics[width=0.23\columnwidth]{\gefcomfigsdir/station1contigsubset_3}
\includegraphics[width=0.23\columnwidth]{\gefcomfigsdir/station1contigsubset_4}
\caption[Model constructed for contiguous subset of temperature data]{
Model constructed for contiguous subset of temperature data. From left to right: Constant offset, daily periodicity and smooth functions.
}
\label{fig:gefcom:contig}
\end{figure}

What we have observed here is that different approximation strategies are appropriate for learning different types of function.
Exactly periodic functions or smooth functions with very long lengthscales can be learnt effectively using a random subset of the data.
In contrast, shorter lengthscale components can be learnt by assuming that the function being learnt is not strongly affected by data far away from where the function is being modelled.
This is representative of a common theme; while there are myriad methods for performing approximate inference in Gaussian processes almost all of them either exploit the fact that long lengthscale components do not require much data to be learnt effectively and short lengthscale components have approximate statistical independence relationships that allow for local approximate inference methods.
The above example demonstrates that approximate inference methods may need to deal with these two types of structures simultaneously.

Continuing with the theme of using a subset of data one could construct a subset of data which is a mixture of the two methods already described.
Using this data subset results in a model composed of
\begin{itemize}
  \item A very smooth function
  \item A periodic function with a period of 1.0 years
  \item An approximately periodic function with a period of 24.0 hours
  \item A rapidly varying smooth function with a lengthscale of 19.5 hours
  \item A rapidly varying smooth function with a lengthscale of 2.0 hours
  \item Uncorrelated noise
\end{itemize}
By using this subset the kernel search has been able to infer both periodicities and smooth functions at multiple lengthscales.
However, using my own prior knowledge about temperature dynamics a more complete description of the data might be
\[
  & \textrm{ An annual periodic function} \\
  + & \textrm{ A long term smooth function } \\
  + & \textrm{ Shorter term smooth functions } \\
  + & \textrm{ A daily approximately periodic function} \\
  + & \textrm{ A daily periodic function which is similar to values in different years}
\]
where the final term represents that the average daily profile of temperatures might change shape and size depending on the time of year.
The last term could be modelled with a Gaussian process using a kernel of the form $\kSE \kerntimes \kPer \kerntimes \kPer$\footnotemark{}.
The most effective subset of data, or more generally type of approximation scheme, is different for these different functions.
The periodicity and long term smooth function can be learned with a random subset of the data.
The shorter term smooth functions will require more data to be learned effectively but one will be able to make use of approximate statistical independence relationships between far away data points to learn the function at any point using local data.
For the final function, to learn the function at any point one only needs to see data nearby in time and the corresponding time points in previous years.
\footnotetext{
This term was not inferred on any subsets of data.
It is possible that the data does not actually display this changing periodicity pattern that I expect; it seems more likely however that 500 data points were insufficient evidence for such a complicated structure to be inferred.
}

It has previously been remarked that approximate inference strategies should be able to capture both global and local functions \citep[e.g.][]{Snelson2007-mj, Vanhatalo2012-yr}.
However, I am unaware of any approximate inference strategy that attempts to exploit more than two lengthscales simultaneously.
It seems reasonable that approximate inference strategies that exploit all aspects of structured kernels would make for profitable research.

\section{Conclusion}

In this competition I applied general purpose machine learning / regression algorithms and made few adjustments for the specific domain the data arose from.
Despite this, the resulting predictions were highly competitive.
It is hoped that this report shows that simple techniques can be very effective for predictive modelling and can inspire practical techniques for load forecasting.

With the benefit of hindsight I also demonstrated that some of the models used in this competition could have been automatically constructed using the methods described in chapters~\ref{ch:construction}~and~\ref{ch:description}.
However, to do so required a careful choice of approximate inference strategy and revealed a potential area for future research in approximate inference methods for Gaussian processes.

\outbpdocument{
\bibliographystyle{plainnat}
\bibliography{references.bib}
}
