`@INPROCEEDINGS{Hoff2007-ja,
  title     = "Modeling homophily and stochastic equivalence in symmetric
               relational data",
  booktitle = "Advances in Neural Information Processing Systems ({NIPS})",
  author    = "Hoff, Peter D.",
  volume    =  20,
  pages     = "657--664",
  year      =  2007
}
@INPROCEEDINGS{Roy2009-ge,
  title     = "The Mondrian process",
  booktitle = "Advances in Neural Information Processing Systems ({NIPS})",
  author    = "Roy, Daniel M. and Teh, Yee Whye",
  abstract  = "We describe a novel class of distributions, called Mondrian
               processes, which can be interpreted as probability distributions
               over kd-tree data structures. Mon- drian processes are
               multidimensional generalizations of Poisson processes and this
               connection allows us to construct multidimensional
               generalizations of the stick- breaking process described by
               Sethuraman (1994), recovering the Dirichlet pro- cess in one
               dimension. After introducing the Aldous-Hoover representation
               for jointly and separately exchangeable arrays, we show how the
               process can be used as a nonparametric prior distribution in
               Bayesian models of relational data.",
  publisher = "Citeseer",
  year      =  2009
}
@INPROCEEDINGS{Lloyd2012-sb,
  title     = "Random function priors for exchangeable graphs and arrays",
  booktitle = "Advances in Neural Information Processing Systems ({NIPS})",
  author    = "Lloyd, James Robert and Orbanz, Peter and Roy, Daniel M. and
               Ghahramani, Zoubin",
  year      =  2012
}
@UNPUBLISHED{Wang2012-rc,
  title         = "Gaussian Process Regression with Heteroscedastic or
                   {Non-Gaussian} Residuals",
  author        = "Wang, Chunyi and Neal, Radford M",
  abstract      = "Gaussian Process (GP) regression models typically assume
                   that residuals are Gaussian and have the same variance for
                   all observations. However, applications with input-dependent
                   noise (heteroscedastic residuals) frequently arise in
                   practice, as do applications in which the residuals do not
                   have a Gaussian distribution. In this paper, we propose a GP
                   Regression model with a latent variable that serves as an
                   additional unobserved covariate for the regression. This
                   model (which we call GPLC) allows for heteroscedasticity
                   since it allows the function to have a changing partial
                   derivative with respect to this unobserved covariate. With a
                   suitable covariance function, our GPLC model can handle (a)
                   Gaussian residuals with input-dependent variance, or (b)
                   non-Gaussian residuals with input-dependent variance, or (c)
                   Gaussian residuals with constant variance. We compare our
                   model, using synthetic datasets, with a model proposed by
                   Goldberg, Williams and Bishop (1998), which we refer to as
                   GPLV, which only deals with case (a), as well as a standard
                   GP model which can handle only case (c). Markov Chain Monte
                   Carlo methods are developed for both modelsl. Experiments
                   show that when the data is heteroscedastic, both GPLC and
                   GPLV give better results (smaller mean squared error and
                   negative log-probability density) than standard GP
                   regression. In addition, when the residual are Gaussian, our
                   GPLC model is generally nearly as good as GPLV, while when
                   the residuals are non-Gaussian, our GPLC model is better
                   than GPLV.",
  month         =  "26~" # dec,
  year          =  2012,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1212.6246"
}
@ARTICLE{Friedman1999-mo,
  title     = "Learning probabilistic relational models",
  author    = "Friedman, N and Getoor, L and Koller, D and Pfeffer, A",
  abstract  = "Abstract A large portion of real-world data is stored in
               commercial relational database systems. In contrast, most
               statistical learning methods work only with “flat” data
               representations. Thus, to apply these methods, we are forced to
               convert our data into a flat ...",
  journal   = "IJCAI",
  publisher = "robotics.stanford.edu",
  year      =  1999
}