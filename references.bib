`@INPROCEEDINGS{Hoff2007-ja,
  title     = "Modeling homophily and stochastic equivalence in symmetric
               relational data",
  booktitle = "Advances in Neural Information Processing Systems ({NIPS})",
  author    = "Hoff, Peter D.",
  volume    =  20,
  pages     = "657--664",
  year      =  2007
}
@INPROCEEDINGS{Roy2009-ge,
  title     = "The Mondrian process",
  booktitle = "Advances in Neural Information Processing Systems ({NIPS})",
  author    = "Roy, Daniel M. and Teh, Yee Whye",
  abstract  = "We describe a novel class of distributions, called Mondrian
               processes, which can be interpreted as probability distributions
               over kd-tree data structures. Mon- drian processes are
               multidimensional generalizations of Poisson processes and this
               connection allows us to construct multidimensional
               generalizations of the stick- breaking process described by
               Sethuraman (1994), recovering the Dirichlet pro- cess in one
               dimension. After introducing the Aldous-Hoover representation
               for jointly and separately exchangeable arrays, we show how the
               process can be used as a nonparametric prior distribution in
               Bayesian models of relational data.",
  publisher = "Citeseer",
  year      =  2009
}
@INPROCEEDINGS{Lloyd2012-sb,
  title     = "Random function priors for exchangeable graphs and arrays",
  booktitle = "Advances in Neural Information Processing Systems ({NIPS})",
  author    = "Lloyd, James Robert and Orbanz, Peter and Roy, Daniel M. and
               Ghahramani, Zoubin",
  year      =  2012
}
@UNPUBLISHED{Wang2012-rc,
  title         = "Gaussian Process Regression with Heteroscedastic or
                   {Non-Gaussian} Residuals",
  author        = "Wang, Chunyi and Neal, Radford M",
  abstract      = "Gaussian Process (GP) regression models typically assume
                   that residuals are Gaussian and have the same variance for
                   all observations. However, applications with input-dependent
                   noise (heteroscedastic residuals) frequently arise in
                   practice, as do applications in which the residuals do not
                   have a Gaussian distribution. In this paper, we propose a GP
                   Regression model with a latent variable that serves as an
                   additional unobserved covariate for the regression. This
                   model (which we call GPLC) allows for heteroscedasticity
                   since it allows the function to have a changing partial
                   derivative with respect to this unobserved covariate. With a
                   suitable covariance function, our GPLC model can handle (a)
                   Gaussian residuals with input-dependent variance, or (b)
                   non-Gaussian residuals with input-dependent variance, or (c)
                   Gaussian residuals with constant variance. We compare our
                   model, using synthetic datasets, with a model proposed by
                   Goldberg, Williams and Bishop (1998), which we refer to as
                   GPLV, which only deals with case (a), as well as a standard
                   GP model which can handle only case (c). Markov Chain Monte
                   Carlo methods are developed for both modelsl. Experiments
                   show that when the data is heteroscedastic, both GPLC and
                   GPLV give better results (smaller mean squared error and
                   negative log-probability density) than standard GP
                   regression. In addition, when the residual are Gaussian, our
                   GPLC model is generally nearly as good as GPLV, while when
                   the residuals are non-Gaussian, our GPLC model is better
                   than GPLV.",
  month         =  "26~" # dec,
  year          =  2012,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1212.6246"
}
@ARTICLE{Friedman1999-mo,
  title     = "Learning probabilistic relational models",
  author    = "Friedman, N and Getoor, L and Koller, D and Pfeffer, A",
  abstract  = "Abstract A large portion of real-world data is stored in
               commercial relational database systems. In contrast, most
               statistical learning methods work only with “flat” data
               representations. Thus, to apply these methods, we are forced to
               convert our data into a flat ...",
  journal   = "IJCAI",
  publisher = "robotics.stanford.edu",
  year      =  1999
}
@ARTICLE{Kolda2009-ba,
  title    = "Tensor Decompositions and Applications",
  author   = "Kolda, Tamara G. and Bader, Brett W.",
  journal  = "SIAM Rev.",
  volume   =  51,
  number   =  3,
  pages    = "455--500",
  month    =  aug,
  year     =  2009,
  keywords = "15a69; 65f99; ams subject classifications; candecomp; canonical
              decomposition; higher-order principal components analysis;
              higher-order singular value decomposition; hosvd; multilinear
              algebra; multiway arrays; parafac; parallel factors; tensor
              decompositions; tucker"
}
@ARTICLE{Menon2011-ku,
  title    = "Link prediction via matrix factorization",
  author   = "Menon, A",
  journal  = "Machine Learning and Knowledge Discovery in",
  pages    = "437--452",
  year     =  2011,
  keywords = "link prediction; matrix factorization; side information"
}
@ARTICLE{Hoffman_undated-ri,
  title  = "dyadic.pdf",
  author = "Hoffman, Thomas and Puzicha, Jan and Jordan, Michaeli"
}
@ARTICLE{Hong2014-yf,
  title    = "Global Energy Forecasting Competition 2012",
  author   = "Hong, Tao and Pinson, Pierre and Fan, Shu",
  abstract = "Abstract The Global Energy Forecasting Competition (GEFCom2012)
              attracted hundreds of participants worldwide, who contributed
              many novel ideas to the energy forecasting field. This paper
              introduces both tracks of GEFCom2012, hierarchical load
              forecasting and wind power forecasting, with details on the
              aspects of the problem, the data, and a summary of the methods
              used by selected top entries. We also discuss the lessons learned
              from this competition from the organizers’ perspective. The
              complete data set, including the solution data, is published
              along with this paper, in an effort to establish a benchmark data
              pool for the community.",
  journal  = "Int. J. Forecast.",
  volume   =  30,
  number   =  2,
  pages    = "357--363",
  month    =  apr,
  year     =  2014
}
@BOOK{Hastie2009-hj,
  title     = "The Elements of Statistical Learning",
  author    = "Hastie, Trevor and Tibshirani, Rob and Friedman, Jerome H.",
  publisher = "Springer",
  year      =  2009
}
@ARTICLE{Hoeting1999-tn,
  title   = "Bayesian Model Averaging: A Tutorial",
  author  = "Hoeting, Jennifer A. and Madigan, David and Raftery, Adrian E. and
             Volinsky, Chris T.",
  journal = "Stat. Sci.",
  volume  =  14,
  number  =  4,
  pages   = "382--401",
  year    =  1999
}
@INPROCEEDINGS{Osborne2009-ti,
  title     = "Gaussian processes for global optimization",
  booktitle = "3rd International Conference on Learning and Intelligent
               Optimization ({LION3})",
  author    = "Osborne, Michael A. and Garnett, Roman and Roberts, Stephen J.",
  abstract  = "We introduce a novel Bayesian approach to global optimiza- tion
               using Gaussian processes. We frame the optimization of both
               noisy and noiseless functions as sequential decision problems,
               and introduce myopic and non-myopic solutions to them. Here our
               solutions can be tai- lored to exactly the degree of confidence
               we require of them. The use of Gaussian processes allows us to
               benefit from the incorporation of prior knowledge about our
               objective function, and also from any derivative observations.
               Using this latter fact, we introduce an innovative method to
               combat conditioning problems. Our algorithm demonstrates a
               signif- icant improvement over its competitors in overall
               performance across a wide range of canonical test problems.",
  year      =  2009,
  keywords  = "bayesian methods; decision theory; gaussian processes; global
               optimization; noisy optimization; non-convex optimization"
}
@ARTICLE{Snoek2012-ri,
  title   = "Practical Bayesian Optimization of Machine Learning Algorithms",
  author  = "Snoek, J and Larochelle, H and Adams, R P",
  journal = "arXiv preprint arXiv:1206.2944",
  year    =  2012
}