`@INPROCEEDINGS{Hoff2007-ja,
  title     = "Modeling homophily and stochastic equivalence in symmetric
               relational data",
  booktitle = "Advances in Neural Information Processing Systems ({NIPS})",
  author    = "Hoff, Peter D.",
  volume    =  20,
  pages     = "657--664",
  year      =  2007
}
@INPROCEEDINGS{Roy2009-ge,
  title     = "The Mondrian process",
  booktitle = "Advances in Neural Information Processing Systems ({NIPS})",
  author    = "Roy, Daniel M. and Teh, Yee Whye",
  abstract  = "We describe a novel class of distributions, called Mondrian
               processes, which can be interpreted as probability distributions
               over kd-tree data structures. Mon- drian processes are
               multidimensional generalizations of Poisson processes and this
               connection allows us to construct multidimensional
               generalizations of the stick- breaking process described by
               Sethuraman (1994), recovering the Dirichlet pro- cess in one
               dimension. After introducing the Aldous-Hoover representation
               for jointly and separately exchangeable arrays, we show how the
               process can be used as a nonparametric prior distribution in
               Bayesian models of relational data.",
  publisher = "Citeseer",
  year      =  2009
}
@INPROCEEDINGS{Lloyd2012-sb,
  title     = "Random function priors for exchangeable graphs and arrays",
  booktitle = "Advances in Neural Information Processing Systems ({NIPS})",
  author    = "Lloyd, James Robert and Orbanz, Peter and Roy, Daniel M. and
               Ghahramani, Zoubin",
  year      =  2012
}
@UNPUBLISHED{Wang2012-rc,
  title         = "Gaussian Process Regression with Heteroscedastic or
                   {Non-Gaussian} Residuals",
  author        = "Wang, Chunyi and Neal, Radford M",
  abstract      = "Gaussian Process (GP) regression models typically assume
                   that residuals are Gaussian and have the same variance for
                   all observations. However, applications with input-dependent
                   noise (heteroscedastic residuals) frequently arise in
                   practice, as do applications in which the residuals do not
                   have a Gaussian distribution. In this paper, we propose a GP
                   Regression model with a latent variable that serves as an
                   additional unobserved covariate for the regression. This
                   model (which we call GPLC) allows for heteroscedasticity
                   since it allows the function to have a changing partial
                   derivative with respect to this unobserved covariate. With a
                   suitable covariance function, our GPLC model can handle (a)
                   Gaussian residuals with input-dependent variance, or (b)
                   non-Gaussian residuals with input-dependent variance, or (c)
                   Gaussian residuals with constant variance. We compare our
                   model, using synthetic datasets, with a model proposed by
                   Goldberg, Williams and Bishop (1998), which we refer to as
                   GPLV, which only deals with case (a), as well as a standard
                   GP model which can handle only case (c). Markov Chain Monte
                   Carlo methods are developed for both modelsl. Experiments
                   show that when the data is heteroscedastic, both GPLC and
                   GPLV give better results (smaller mean squared error and
                   negative log-probability density) than standard GP
                   regression. In addition, when the residual are Gaussian, our
                   GPLC model is generally nearly as good as GPLV, while when
                   the residuals are non-Gaussian, our GPLC model is better
                   than GPLV.",
  month         =  "26~" # dec,
  year          =  2012,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1212.6246"
}
@ARTICLE{Friedman1999-mo,
  title     = "Learning probabilistic relational models",
  author    = "Friedman, N and Getoor, L and Koller, D and Pfeffer, A",
  abstract  = "Abstract A large portion of real-world data is stored in
               commercial relational database systems. In contrast, most
               statistical learning methods work only with “flat” data
               representations. Thus, to apply these methods, we are forced to
               convert our data into a flat ...",
  journal   = "IJCAI",
  publisher = "robotics.stanford.edu",
  year      =  1999
}
@ARTICLE{Kolda2009-ba,
  title    = "Tensor Decompositions and Applications",
  author   = "Kolda, Tamara G. and Bader, Brett W.",
  journal  = "SIAM Rev.",
  volume   =  51,
  number   =  3,
  pages    = "455--500",
  month    =  aug,
  year     =  2009,
  keywords = "15a69; 65f99; ams subject classifications; candecomp; canonical
              decomposition; higher-order principal components analysis;
              higher-order singular value decomposition; hosvd; multilinear
              algebra; multiway arrays; parafac; parallel factors; tensor
              decompositions; tucker"
}
@ARTICLE{Menon2011-ku,
  title    = "Link prediction via matrix factorization",
  author   = "Menon, A",
  journal  = "Machine Learning and Knowledge Discovery in",
  pages    = "437--452",
  year     =  2011,
  keywords = "link prediction; matrix factorization; side information"
}
@ARTICLE{Hoffman_undated-ri,
  title  = "dyadic.pdf",
  author = "Hoffman, Thomas and Puzicha, Jan and Jordan, Michaeli"
}
@ARTICLE{Hong2014-yf,
  title    = "Global Energy Forecasting Competition 2012",
  author   = "Hong, Tao and Pinson, Pierre and Fan, Shu",
  abstract = "Abstract The Global Energy Forecasting Competition (GEFCom2012)
              attracted hundreds of participants worldwide, who contributed
              many novel ideas to the energy forecasting field. This paper
              introduces both tracks of GEFCom2012, hierarchical load
              forecasting and wind power forecasting, with details on the
              aspects of the problem, the data, and a summary of the methods
              used by selected top entries. We also discuss the lessons learned
              from this competition from the organizers’ perspective. The
              complete data set, including the solution data, is published
              along with this paper, in an effort to establish a benchmark data
              pool for the community.",
  journal  = "Int. J. Forecast.",
  volume   =  30,
  number   =  2,
  pages    = "357--363",
  month    =  apr,
  year     =  2014
}
@BOOK{Hastie2009-hj,
  title     = "The Elements of Statistical Learning",
  author    = "Hastie, Trevor and Tibshirani, Rob and Friedman, Jerome H.",
  publisher = "Springer",
  year      =  2009
}
@ARTICLE{Hoeting1999-tn,
  title   = "Bayesian Model Averaging: A Tutorial",
  author  = "Hoeting, Jennifer A. and Madigan, David and Raftery, Adrian E. and
             Volinsky, Chris T.",
  journal = "Stat. Sci.",
  volume  =  14,
  number  =  4,
  pages   = "382--401",
  year    =  1999
}
@INPROCEEDINGS{Osborne2009-ti,
  title     = "Gaussian processes for global optimization",
  booktitle = "3rd International Conference on Learning and Intelligent
               Optimization ({LION3})",
  author    = "Osborne, Michael A. and Garnett, Roman and Roberts, Stephen J.",
  abstract  = "We introduce a novel Bayesian approach to global optimiza- tion
               using Gaussian processes. We frame the optimization of both
               noisy and noiseless functions as sequential decision problems,
               and introduce myopic and non-myopic solutions to them. Here our
               solutions can be tai- lored to exactly the degree of confidence
               we require of them. The use of Gaussian processes allows us to
               benefit from the incorporation of prior knowledge about our
               objective function, and also from any derivative observations.
               Using this latter fact, we introduce an innovative method to
               combat conditioning problems. Our algorithm demonstrates a
               signif- icant improvement over its competitors in overall
               performance across a wide range of canonical test problems.",
  year      =  2009,
  keywords  = "bayesian methods; decision theory; gaussian processes; global
               optimization; noisy optimization; non-convex optimization"
}
@ARTICLE{Snoek2012-ri,
  title   = "Practical Bayesian Optimization of Machine Learning Algorithms",
  author  = "Snoek, J and Larochelle, H and Adams, R P",
  journal = "arXiv preprint arXiv:1206.2944",
  year    =  2012
}
@INPROCEEDINGS{Duvenaud2013-dn,
  title       = "Structure Discovery in Nonparametric Regression through
                 Compositional Kernel Search",
  booktitle   = "Proceedings of the 30th International Conference on Machine
                 Learning",
  author      = "Duvenaud, David and Lloyd, James Robert and Grosse, Roger and
                 Tenenbaum, Joshua B and Ghahramani, Zoubin",
  institution = "Department of Engineering, University of Cambridge",
  month       =  jun,
  year        =  2013
}
@ARTICLE{Quinonero-Candela2005-er,
  title     = "A unifying view of sparse approximate Gaussian process
               regression",
  author    = "Qui\~{n}onero-Candela, J and Rasmussen, C E",
  journal   = "J. Mach. Learn. Res.",
  publisher = "MIT Press",
  volume    =  6,
  pages     = "1939--1959",
  year      =  2005,
  keywords  = "bayesian committee; gaussian process; probabilistic regression;
               sparse approximation"
}
@BOOK{Rasmussen2006-ml,
  title     = "Gaussian Processes for Machine Learning",
  author    = "Rasmussen, C E and Williams, C K",
  publisher = "The MIT Press, Cambridge, MA, USA",
  year      =  2006
}
@INPROCEEDINGS{Todorovski1997-st,
  title     = "Declarative Bias in Equation Discovery",
  booktitle = "International Conference on Machine Learning",
  author    = "Todorovski, L and Dzeroski, S",
  pages     = "376--384",
  year      =  1997
}
@INPROCEEDINGS{Grosse2012-zi,
  title     = "Exploiting compositionality to explore a large space of model
               structures",
  booktitle = "Conf. on Unc. in Art. Int. ({UAI})",
  author    = "Grosse, Roger B and Salakhutdinov, Ruslan and Freeman, William T
               and Tenenbaum, Joshua B",
  year      =  2012
}
@BOOK{Hastie1990-ay,
  title     = "Generalized additive models",
  author    = "Hastie, T J and Tibshirani, R J",
  publisher = "Chapman \& Hall/CRC",
  year      =  1990
}
@ARTICLE{LeCun1989-ba,
  title     = "Backpropagation applied to handwritten zip code recognition",
  author    = "LeCun, Y and Boser, B and Denker, J S and Henderson, D and
               Howard, R E and Hubbard, W and Jackel, L D",
  journal   = "Neural Comput.",
  publisher = "Massachusetts Institute of Technology",
  volume    =  1,
  pages     = "541--551",
  year      =  1989
}
@INPROCEEDINGS{Poon2011-sc,
  title     = "Sum-product networks: a new deep architecture",
  booktitle = "Conference on Uncertainty in {AI}",
  author    = "Poon, H and Domingos, P",
  year      =  2011
}
@INPROCEEDINGS{Rasmussen2001-rv,
  title     = "Occam's razor",
  booktitle = "Advances in Neural Information Processing Systems",
  author    = "Rasmussen, C E and Ghahramani, Z",
  year      =  2001
}
@ARTICLE{Schwarz1978-wp,
  title     = "Estimating the dimension of a model",
  author    = "Schwarz, G",
  journal   = "Ann. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  6,
  number    =  2,
  pages     = "461--464",
  year      =  1978
}
@INPROCEEDINGS{Duvenaud2011-wb,
  title     = "Additive Gaussian Processes",
  booktitle = "Advances in Neural Information Processing Systems",
  author    = "Duvenaud, D and Nickisch, H and Rasmussen, C E",
  year      =  2011
}
@ARTICLE{Plate1999-xh,
  title     = "Accuracy versus interpretability in flexible modeling:
               Implementing a tradeoff using Gaussian process models",
  author    = "Plate, T A",
  journal   = "Behaviormetrika",
  publisher = "Citeseer",
  volume    =  26,
  pages     = "29--50",
  year      =  1999
}
@UNPUBLISHED{Wahba2004-fk,
  title         = "An introduction to (smoothing spline) {ANOVA} models in
                   {RKHS} with examples in geographical data, medicine,
                   atmospheric science and machine learning",
  author        = "Wahba, Grace",
  abstract      = "Smoothing Spline ANOVA (SS-ANOVA) models in reproducing
                   kernel Hilbert spaces (RKHS) provide a very general
                   framework for data analysis, modeling and learning in a
                   variety of fields. Discrete, noisy scattered, direct and
                   indirect observations can be accommodated with multiple
                   inputs and multiple possibly correlated outputs and a
                   variety of meaningful structures. The purpose of this paper
                   is to give a brief overview of the approach and describe and
                   contrast a series of applications, while noting some recent
                   results.",
  month         =  "19~" # oct,
  year          =  2004,
  archivePrefix = "arXiv",
  primaryClass  = "math.ST",
  eprint        = "math/0410419"
}
@BOOK{Gu2002-at,
  title     = "Smoothing spline {ANOVA} models",
  author    = "Gu, C",
  publisher = "Springer Verlag",
  year      =  2002
}
@BOOK{Wahba1990-ml,
  title     = "Spline models for observational data",
  author    = "Wahba, G",
  publisher = "Society for Industrial Mathematics",
  year      =  1990
}
@BOOK{Ruppert2003-uq,
  title     = "Semiparametric regression",
  author    = "Ruppert, D and Wand, M P and Carroll, R J",
  publisher = "Cambridge University Press",
  volume    =  12,
  year      =  2003
}
@ARTICLE{Christoudias2009-an,
  title     = "Bayesian localized multiple kernel learning",
  author    = "Christoudias, M and Urtasun, R and Darrell, T",
  journal   = "Technical report, EECS Department, University of California,
               Berkeley",
  publisher = "EECS Dept., University of California, Berkeley, 2009",
  year      =  2009
}
@INBOOK{Bach2009-hr,
  title  = "Exploring Large Feature Spaces with Hierarchical Multiple Kernel
            Learning",
  author = "Bach, F",
  pages  = "105--112",
  year   =  2009
}
@ARTICLE{Lawrence2005-cn,
  title     = "Probabilistic non-linear principal component analysis with
               Gaussian process latent variable models",
  author    = "Lawrence, Neil D.",
  journal   = "J. Mach. Learn. Res.",
  publisher = "JMLR. org",
  volume    =  6,
  pages     = "1783--1816",
  year      =  2005,
  keywords  = "gaussian processes; latent variable models; methods; principal
               component analysis; spectral; unsupervised learning;
               visualisation"
}
@ARTICLE{Salakhutdinov2008-zt,
  title   = "Using deep belief nets to learn covariance kernels for Gaussian
             processes",
  author  = "Salakhutdinov, Ruslan and Hinton, Geoffrey",
  journal = "Adv. Neural Inf. Process. Syst.",
  volume  =  20,
  pages   = "1249--1256",
  year    =  2008
}
@INPROCEEDINGS{Diosan2007-un,
  title       = "Evolving kernel functions for {SVMs} by genetic programming",
  booktitle   = "Machine Learning and Applications, 2007",
  author      = "Diosan, L and Rogozan, A and Pecuchet, J P",
  pages       = "19--24",
  institution = "IEEE",
  year        =  2007
}
@INPROCEEDINGS{Wilson2013-eq,
  title     = "Gaussian Process Covariance Kernels for Pattern Discovery and
               Extrapolation",
  booktitle = "Proc. Int. Conf. Machine Learn.",
  author    = "Wilson, Andrew Gordon and Adams, Ryan Prescott",
  year      =  2013
}
@INPROCEEDINGS{Bing2010-of,
  title     = "A {GP-based} kernel construction and optimization method for
               {RVM}",
  booktitle = "International Conference on Computer and Automation Engineering
               ({ICCAE})",
  author    = "Bing, W and Wen-qiong, Z and Ling, C and Jia-hong, L",
  volume    =  4,
  pages     = "419--423",
  year      =  2010
}
@ARTICLE{Schmidt2009-if,
  title     = "Distilling free-form natural laws from experimental data",
  author    = "Schmidt, Michael and Lipson, Hod",
  abstract  = "For centuries, scientists have attempted to identify and
               document analytical laws that underlie physical phenomena in
               nature. Despite the prevalence of computing power, the process
               of finding natural laws and their corresponding equations has
               resisted automation. A key challenge to finding analytic
               relations automatically is defining algorithmically what makes a
               correlation in observed data important and insightful. We
               propose a principle for the identification of nontriviality. We
               demonstrated this approach by automatically searching
               motion-tracking data captured from various physical systems,
               ranging from simple harmonic oscillators to chaotic
               double-pendula. Without any prior knowledge about physics,
               kinematics, or geometry, the algorithm discovered Hamiltonians,
               Lagrangians, and other laws of geometric and momentum
               conservation. The discovery rate accelerated as laws found for
               simpler systems were used to bootstrap explanations for more
               complex systems, gradually uncovering the ``alphabet'' used to
               describe those systems.",
  journal   = "Science",
  publisher = "American Association for the Advancement of Science",
  volume    =  324,
  number    =  5923,
  pages     = "81--85",
  month     =  apr,
  year      =  2009,
  keywords  = "Algorithms; Artificial Intelligence; Mathematical Concepts;
               Nonlinear Dynamics; Physical Processes; Regression Analysis;
               Software"
}
@INPROCEEDINGS{Washio1999-vy,
  title     = "Discovering admissible model equations from observed data based
               on scale-types and identity constraints",
  booktitle = "International Joint Conference On Artifical Intelligence",
  author    = "Washio, T and Motoda, H and Niwa, Y and {Others}",
  volume    =  16,
  pages     = "772--779",
  year      =  1999
}
@ARTICLE{Kemp2008-ye,
  title     = "The discovery of structural form",
  author    = "Kemp, Charles and Tenenbaum, Joshua B",
  abstract  = "Algorithms for finding structure in data have become
               increasingly important both as tools for scientific data
               analysis and as models of human learning, yet they suffer from a
               critical limitation. Scientists discover qualitatively new forms
               of structure in observed data: For instance, Linnaeus recognized
               the hierarchical organization of biological species, and
               Mendeleev recognized the periodic structure of the chemical
               elements. Analogous insights play a pivotal role in cognitive
               development: Children discover that object category labels can
               be organized into hierarchies, friendship networks are organized
               into cliques, and comparative relations (e.g., ``bigger than''
               or ``better than'') respect a transitive order. Standard
               algorithms, however, can only learn structures of a single form
               that must be specified in advance: For instance, algorithms for
               hierarchical clustering create tree structures, whereas
               algorithms for dimensionality-reduction create low-dimensional
               spaces. Here, we present a computational model that learns
               structures of many different forms and that discovers which form
               is best for a given dataset. The model makes probabilistic
               inferences over a space of graph grammars representing trees,
               linear orders, multidimensional spaces, rings, dominance
               hierarchies, cliques, and other forms and successfully discovers
               the underlying structure of a variety of physical, biological,
               and social domains. Our approach brings structure learning
               methods closer to human abilities and may lead to a deeper
               computational understanding of cognitive development.",
  journal   = "Proceedings of the National Academy of Sciences",
  publisher = "National Acad Sciences",
  volume    =  105,
  number    =  31,
  pages     = "10687--10692",
  month     =  aug,
  year      =  2008,
  keywords  = "Algorithms; Data Interpretation; Humans; Learning; Learning:
               physiology; Models; Research Design; Statistical; Theoretical"
}
@BOOK{Box1976-qk,
  title  = "Time series analysis: forecasting and control",
  author = "Box, G E P and Jenkins, G M and Reinsel, G C",
  year   =  1976
}
@INPROCEEDINGS{Lloyd2014-nz,
  title         = "Automatic Construction and {Natural-Language} Description of
                   Nonparametric Regression Models",
  booktitle     = "Association for the Advancement of Artificial Intelligence
                   ({AAAI})",
  author        = "Lloyd, James Robert and Duvenaud, David and Grosse, Roger
                   and Tenenbaum, Joshua B and Ghahramani, Zoubin",
  abstract      = "This paper presents the beginnings of an automatic
                   statistician, focusing on regression problems. Our system
                   explores an open-ended space of possible statistical models
                   to discover a good explanation of the data, and then
                   produces a detailed report with figures and natural-language
                   text. Our approach treats unknown functions
                   nonparametrically using Gaussian processes, which has two
                   important consequences. First, Gaussian processes model
                   functions in terms of high-level properties (e.g.
                   smoothness, trends, periodicity, changepoints). Taken
                   together with the compositional structure of our language of
                   models, this allows us to automatically describe functions
                   through a decomposition into additive parts. Second, the use
                   of flexible nonparametric models and a rich language for
                   composing them in an open-ended manner also results in
                   state-of-the-art extrapolation performance evaluated over 13
                   real time series data sets from various domains.",
  month         =  jul,
  year          =  2014,
  conference    = "Association for the Advancement of Artificial Intelligence
                   (AAAI)",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML"
}
@ARTICLE{Lean1995-vp,
  title     = "Reconstruction of solar irradiance since 1610: Implications for
               climate change",
  author    = "Lean, J and Beer, J and Bradley, R",
  journal   = "Geophys. Res. Lett.",
  publisher = "AGU AMERICAN GEOPHYSICAL UNION",
  volume    =  22,
  number    =  23,
  pages     = "3195--3198",
  year      =  1995
}
@ARTICLE{Kronberger_undated-vf,
  title    = "Evolution of Covariance Functions for Gaussian Process Regression
              using Genetic Programming",
  author   = "Kronberger, Gabriel and Kommenda, Michael",
  keywords = "gaussian process; genetic programming; structure identification"
}
@ARTICLE{Barbu2012-wv,
  title    = "An Introduction to Artificial Prediction Markets for
              Classification",
  author   = "Barbu, Adrian and Lay, Nathan",
  journal  = "J. Mach. Learn. Res.",
  volume   =  13,
  pages    = "2177--2204",
  year     =  2012,
  keywords = "ensemble methods; implicit on-; online learning; random forest;
              supervised learning"
}
@ARTICLE{Ganesalingam_undated-us,
  title  = "A fully automatic problem solver with human-style output",
  author = "Ganesalingam, M and Gowers, W T",
  pages  = "1--42"
}