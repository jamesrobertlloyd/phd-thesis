`@INPROCEEDINGS{Hoff2007-ja,
  title     = "Modeling homophily and stochastic equivalence in symmetric
               relational data",
  booktitle = "Advances in Neural Information Processing Systems ({NIPS})",
  author    = "Hoff, Peter D.",
  volume    =  20,
  pages     = "657--664",
  year      =  2007
}
@INPROCEEDINGS{Roy2009-ge,
  title     = "The Mondrian process",
  booktitle = "Advances in Neural Information Processing Systems ({NIPS})",
  author    = "Roy, Daniel M. and Teh, Yee Whye",
  abstract  = "We describe a novel class of distributions, called Mondrian
               processes, which can be interpreted as probability distributions
               over kd-tree data structures. Mon- drian processes are
               multidimensional generalizations of Poisson processes and this
               connection allows us to construct multidimensional
               generalizations of the stick- breaking process described by
               Sethuraman (1994), recovering the Dirichlet pro- cess in one
               dimension. After introducing the Aldous-Hoover representation
               for jointly and separately exchangeable arrays, we show how the
               process can be used as a nonparametric prior distribution in
               Bayesian models of relational data.",
  publisher = "Citeseer",
  year      =  2009
}
@INPROCEEDINGS{Lloyd2012-sb,
  title     = "Random function priors for exchangeable graphs and arrays",
  booktitle = "Advances in Neural Information Processing Systems ({NIPS})",
  author    = "Lloyd, James Robert and Orbanz, Peter and Roy, Daniel M. and
               Ghahramani, Zoubin",
  year      =  2012
}
@UNPUBLISHED{Wang2012-rc,
  title         = "Gaussian Process Regression with Heteroscedastic or
                   {Non-Gaussian} Residuals",
  author        = "Wang, Chunyi and Neal, Radford M",
  abstract      = "Gaussian Process (GP) regression models typically assume
                   that residuals are Gaussian and have the same variance for
                   all observations. However, applications with input-dependent
                   noise (heteroscedastic residuals) frequently arise in
                   practice, as do applications in which the residuals do not
                   have a Gaussian distribution. In this paper, we propose a GP
                   Regression model with a latent variable that serves as an
                   additional unobserved covariate for the regression. This
                   model (which we call GPLC) allows for heteroscedasticity
                   since it allows the function to have a changing partial
                   derivative with respect to this unobserved covariate. With a
                   suitable covariance function, our GPLC model can handle (a)
                   Gaussian residuals with input-dependent variance, or (b)
                   non-Gaussian residuals with input-dependent variance, or (c)
                   Gaussian residuals with constant variance. We compare our
                   model, using synthetic datasets, with a model proposed by
                   Goldberg, Williams and Bishop (1998), which we refer to as
                   GPLV, which only deals with case (a), as well as a standard
                   GP model which can handle only case (c). Markov Chain Monte
                   Carlo methods are developed for both modelsl. Experiments
                   show that when the data is heteroscedastic, both GPLC and
                   GPLV give better results (smaller mean squared error and
                   negative log-probability density) than standard GP
                   regression. In addition, when the residual are Gaussian, our
                   GPLC model is generally nearly as good as GPLV, while when
                   the residuals are non-Gaussian, our GPLC model is better
                   than GPLV.",
  month         =  "26~" # dec,
  year          =  2012,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1212.6246"
}
@ARTICLE{Friedman1999-mo,
  title     = "Learning probabilistic relational models",
  author    = "Friedman, N and Getoor, L and Koller, D and Pfeffer, A",
  abstract  = "Abstract A large portion of real-world data is stored in
               commercial relational database systems. In contrast, most
               statistical learning methods work only with “flat” data
               representations. Thus, to apply these methods, we are forced to
               convert our data into a flat ...",
  journal   = "IJCAI",
  publisher = "robotics.stanford.edu",
  year      =  1999
}
@ARTICLE{Kolda2009-ba,
  title    = "Tensor Decompositions and Applications",
  author   = "Kolda, Tamara G. and Bader, Brett W.",
  journal  = "SIAM Rev.",
  volume   =  51,
  number   =  3,
  pages    = "455--500",
  month    =  aug,
  year     =  2009,
  keywords = "15a69; 65f99; ams subject classifications; candecomp; canonical
              decomposition; higher-order principal components analysis;
              higher-order singular value decomposition; hosvd; multilinear
              algebra; multiway arrays; parafac; parallel factors; tensor
              decompositions; tucker"
}
@ARTICLE{Menon2011-ku,
  title    = "Link prediction via matrix factorization",
  author   = "Menon, A",
  journal  = "Machine Learning and Knowledge Discovery",
  pages    = "437--452",
  year     =  2011,
  keywords = "link prediction; matrix factorization; side information"
}
@INPROCEEDINGS{Hoffman_undated-ri,
  title     = "Learning from dyadic data",
  booktitle = "Advances in Neural Information Processing Systems",
  author    = "Hoffman, Thomas and Puzicha, Jan and Jordan, Michael",
  year      =  1998
}
@ARTICLE{Hong2014-yf,
  title    = "Global Energy Forecasting Competition 2012",
  author   = "Hong, Tao and Pinson, Pierre and Fan, Shu",
  abstract = "Abstract The Global Energy Forecasting Competition (GEFCom2012)
              attracted hundreds of participants worldwide, who contributed
              many novel ideas to the energy forecasting field. This paper
              introduces both tracks of GEFCom2012, hierarchical load
              forecasting and wind power forecasting, with details on the
              aspects of the problem, the data, and a summary of the methods
              used by selected top entries. We also discuss the lessons learned
              from this competition from the organizers’ perspective. The
              complete data set, including the solution data, is published
              along with this paper, in an effort to establish a benchmark data
              pool for the community.",
  journal  = "Int. J. Forecast.",
  volume   =  30,
  number   =  2,
  pages    = "357--363",
  month    =  apr,
  year     =  2014
}
@BOOK{Hastie2009-hj,
  title     = "The Elements of Statistical Learning",
  author    = "Hastie, Trevor and Tibshirani, Rob and Friedman, Jerome H.",
  publisher = "Springer",
  year      =  2009
}
@ARTICLE{Hoeting1999-tn,
  title   = "Bayesian Model Averaging: A Tutorial",
  author  = "Hoeting, Jennifer A. and Madigan, David and Raftery, Adrian E. and
             Volinsky, Chris T.",
  journal = "Stat. Sci.",
  volume  =  14,
  number  =  4,
  pages   = "382--401",
  year    =  1999
}
@INPROCEEDINGS{Osborne2009-ti,
  title     = "Gaussian processes for global optimization",
  booktitle = "3rd International Conference on Learning and Intelligent
               Optimization ({LION3})",
  author    = "Osborne, Michael A. and Garnett, Roman and Roberts, Stephen J.",
  abstract  = "We introduce a novel Bayesian approach to global optimiza- tion
               using Gaussian processes. We frame the optimization of both
               noisy and noiseless functions as sequential decision problems,
               and introduce myopic and non-myopic solutions to them. Here our
               solutions can be tai- lored to exactly the degree of confidence
               we require of them. The use of Gaussian processes allows us to
               benefit from the incorporation of prior knowledge about our
               objective function, and also from any derivative observations.
               Using this latter fact, we introduce an innovative method to
               combat conditioning problems. Our algorithm demonstrates a
               signif- icant improvement over its competitors in overall
               performance across a wide range of canonical test problems.",
  year      =  2009,
  keywords  = "bayesian methods; decision theory; gaussian processes; global
               optimization; noisy optimization; non-convex optimization"
}
@ARTICLE{Snoek2012-ri,
  title   = "Practical Bayesian Optimization of Machine Learning Algorithms",
  author  = "Snoek, J and Larochelle, H and Adams, R P",
  journal = "arXiv preprint arXiv:1206.2944",
  year    =  2012
}
@INPROCEEDINGS{Duvenaud2013-dn,
  title       = "Structure Discovery in Nonparametric Regression through
                 Compositional Kernel Search",
  booktitle   = "Proceedings of the 30th International Conference on Machine
                 Learning",
  author      = "Duvenaud, David and Lloyd, James Robert and Grosse, Roger and
                 Tenenbaum, Joshua B and Ghahramani, Zoubin",
  institution = "Department of Engineering, University of Cambridge",
  month       =  jun,
  year        =  2013
}
@ARTICLE{Quinonero-Candela2005-er,
  title     = "A unifying view of sparse approximate Gaussian process
               regression",
  author    = "Qui\~{n}onero-Candela, J and Rasmussen, C E",
  journal   = "J. Mach. Learn. Res.",
  publisher = "MIT Press",
  volume    =  6,
  pages     = "1939--1959",
  year      =  2005,
  keywords  = "bayesian committee; gaussian process; probabilistic regression;
               sparse approximation"
}
@BOOK{Rasmussen2006-ml,
  title     = "Gaussian Processes for Machine Learning",
  author    = "Rasmussen, C E and Williams, C K",
  publisher = "The MIT Press, Cambridge, MA, USA",
  year      =  2006
}
@INPROCEEDINGS{Todorovski1997-st,
  title     = "Declarative Bias in Equation Discovery",
  booktitle = "International Conference on Machine Learning",
  author    = "Todorovski, L and Dzeroski, S",
  pages     = "376--384",
  year      =  1997
}
@INPROCEEDINGS{Grosse2012-zi,
  title     = "Exploiting compositionality to explore a large space of model
               structures",
  booktitle = "Conf. on Unc. in Art. Int. ({UAI})",
  author    = "Grosse, Roger B and Salakhutdinov, Ruslan and Freeman, William T
               and Tenenbaum, Joshua B",
  year      =  2012
}
@BOOK{Hastie1990-ay,
  title     = "Generalized additive models",
  author    = "Hastie, T J and Tibshirani, R J",
  publisher = "Chapman \& Hall/CRC",
  year      =  1990
}
@ARTICLE{LeCun1989-ba,
  title     = "Backpropagation applied to handwritten zip code recognition",
  author    = "LeCun, Y and Boser, B and Denker, J S and Henderson, D and
               Howard, R E and Hubbard, W and Jackel, L D",
  journal   = "Neural Comput.",
  publisher = "Massachusetts Institute of Technology",
  volume    =  1,
  pages     = "541--551",
  year      =  1989
}
@INPROCEEDINGS{Poon2011-sc,
  title     = "Sum-product networks: a new deep architecture",
  booktitle = "Conference on Uncertainty in {AI}",
  author    = "Poon, H and Domingos, P",
  year      =  2011
}
@INPROCEEDINGS{Rasmussen2001-rv,
  title     = "Occam's razor",
  booktitle = "Advances in Neural Information Processing Systems",
  author    = "Rasmussen, C E and Ghahramani, Z",
  year      =  2001
}
@ARTICLE{Schwarz1978-wp,
  title     = "Estimating the dimension of a model",
  author    = "Schwarz, G",
  journal   = "Ann. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  6,
  number    =  2,
  pages     = "461--464",
  year      =  1978
}
@INPROCEEDINGS{Duvenaud2011-wb,
  title     = "Additive Gaussian Processes",
  booktitle = "Advances in Neural Information Processing Systems",
  author    = "Duvenaud, D and Nickisch, H and Rasmussen, C E",
  year      =  2011
}
@ARTICLE{Plate1999-xh,
  title     = "Accuracy versus interpretability in flexible modeling:
               Implementing a tradeoff using Gaussian process models",
  author    = "Plate, T A",
  journal   = "Behaviormetrika",
  publisher = "Citeseer",
  volume    =  26,
  pages     = "29--50",
  year      =  1999
}
@UNPUBLISHED{Wahba2004-fk,
  title         = "An introduction to (smoothing spline) {ANOVA} models in
                   {RKHS} with examples in geographical data, medicine,
                   atmospheric science and machine learning",
  author        = "Wahba, Grace",
  abstract      = "Smoothing Spline ANOVA (SS-ANOVA) models in reproducing
                   kernel Hilbert spaces (RKHS) provide a very general
                   framework for data analysis, modeling and learning in a
                   variety of fields. Discrete, noisy scattered, direct and
                   indirect observations can be accommodated with multiple
                   inputs and multiple possibly correlated outputs and a
                   variety of meaningful structures. The purpose of this paper
                   is to give a brief overview of the approach and describe and
                   contrast a series of applications, while noting some recent
                   results.",
  month         =  "19~" # oct,
  year          =  2004,
  archivePrefix = "arXiv",
  primaryClass  = "math.ST",
  eprint        = "math/0410419"
}
@BOOK{Gu2002-at,
  title     = "Smoothing spline {ANOVA} models",
  author    = "Gu, C",
  publisher = "Springer Verlag",
  year      =  2002
}
@BOOK{Wahba1990-ml,
  title     = "Spline models for observational data",
  author    = "Wahba, G",
  publisher = "Society for Industrial Mathematics",
  year      =  1990
}
@BOOK{Ruppert2003-uq,
  title     = "Semiparametric regression",
  author    = "Ruppert, D and Wand, M P and Carroll, R J",
  publisher = "Cambridge University Press",
  year      =  2003
}
@ARTICLE{Christoudias2009-an,
  title     = "Bayesian localized multiple kernel learning",
  author    = "Christoudias, M and Urtasun, R and Darrell, T",
  journal   = "Technical report, EECS Department, University of California,
               Berkeley",
  publisher = "EECS Dept., University of California, Berkeley, 2009",
  year      =  2009
}
@INPROCEEDINGS{Bach2009-hr,
  title     = "Exploring Large Feature Spaces with Hierarchical Multiple Kernel
               Learning",
  booktitle = "Advances in Neural Information Processing Systems",
  author    = "Bach, F",
  pages     = "105--112",
  year      =  2009
}
@ARTICLE{Lawrence2005-cn,
  title     = "Probabilistic non-linear principal component analysis with
               Gaussian process latent variable models",
  author    = "Lawrence, Neil D.",
  journal   = "J. Mach. Learn. Res.",
  publisher = "JMLR. org",
  volume    =  6,
  pages     = "1783--1816",
  year      =  2005,
  keywords  = "gaussian processes; latent variable models; methods; principal
               component analysis; spectral; unsupervised learning;
               visualisation"
}
@ARTICLE{Salakhutdinov2008-zt,
  title   = "Using deep belief nets to learn covariance kernels for Gaussian
             processes",
  author  = "Salakhutdinov, Ruslan and Hinton, Geoffrey",
  journal = "Adv. Neural Inf. Process. Syst.",
  volume  =  20,
  pages   = "1249--1256",
  year    =  2008
}
@INPROCEEDINGS{Diosan2007-un,
  title       = "Evolving kernel functions for {SVMs} by genetic programming",
  booktitle   = "Machine Learning and Applications, 2007",
  author      = "Diosan, L and Rogozan, A and Pecuchet, J P",
  pages       = "19--24",
  institution = "IEEE",
  year        =  2007
}
@INPROCEEDINGS{Wilson2013-eq,
  title     = "Gaussian Process Covariance Kernels for Pattern Discovery and
               Extrapolation",
  booktitle = "Proc. Int. Conf. Machine Learn.",
  author    = "Wilson, Andrew Gordon and Adams, Ryan Prescott",
  year      =  2013
}
@INPROCEEDINGS{Bing2010-of,
  title     = "A {GP-based} kernel construction and optimization method for
               {RVM}",
  booktitle = "International Conference on Computer and Automation Engineering
               ({ICCAE})",
  author    = "Bing, W and Wen-qiong, Z and Ling, C and Jia-hong, L",
  volume    =  4,
  pages     = "419--423",
  year      =  2010
}
@ARTICLE{Schmidt2009-if,
  title     = "Distilling free-form natural laws from experimental data",
  author    = "Schmidt, Michael and Lipson, Hod",
  abstract  = "For centuries, scientists have attempted to identify and
               document analytical laws that underlie physical phenomena in
               nature. Despite the prevalence of computing power, the process
               of finding natural laws and their corresponding equations has
               resisted automation. A key challenge to finding analytic
               relations automatically is defining algorithmically what makes a
               correlation in observed data important and insightful. We
               propose a principle for the identification of nontriviality. We
               demonstrated this approach by automatically searching
               motion-tracking data captured from various physical systems,
               ranging from simple harmonic oscillators to chaotic
               double-pendula. Without any prior knowledge about physics,
               kinematics, or geometry, the algorithm discovered Hamiltonians,
               Lagrangians, and other laws of geometric and momentum
               conservation. The discovery rate accelerated as laws found for
               simpler systems were used to bootstrap explanations for more
               complex systems, gradually uncovering the ``alphabet'' used to
               describe those systems.",
  journal   = "Science",
  publisher = "American Association for the Advancement of Science",
  volume    =  324,
  number    =  5923,
  pages     = "81--85",
  month     =  apr,
  year      =  2009,
  keywords  = "Algorithms; Artificial Intelligence; Mathematical Concepts;
               Nonlinear Dynamics; Physical Processes; Regression Analysis;
               Software"
}
@INPROCEEDINGS{Washio1999-vy,
  title     = "Discovering admissible model equations from observed data based
               on scale-types and identity constraints",
  booktitle = "International Joint Conference On Artifical Intelligence",
  author    = "Washio, T and Motoda, H and Niwa, Y and {Others}",
  volume    =  16,
  pages     = "772--779",
  year      =  1999
}
@ARTICLE{Kemp2008-ye,
  title     = "The discovery of structural form",
  author    = "Kemp, Charles and Tenenbaum, Joshua B",
  abstract  = "Algorithms for finding structure in data have become
               increasingly important both as tools for scientific data
               analysis and as models of human learning, yet they suffer from a
               critical limitation. Scientists discover qualitatively new forms
               of structure in observed data: For instance, Linnaeus recognized
               the hierarchical organization of biological species, and
               Mendeleev recognized the periodic structure of the chemical
               elements. Analogous insights play a pivotal role in cognitive
               development: Children discover that object category labels can
               be organized into hierarchies, friendship networks are organized
               into cliques, and comparative relations (e.g., ``bigger than''
               or ``better than'') respect a transitive order. Standard
               algorithms, however, can only learn structures of a single form
               that must be specified in advance: For instance, algorithms for
               hierarchical clustering create tree structures, whereas
               algorithms for dimensionality-reduction create low-dimensional
               spaces. Here, we present a computational model that learns
               structures of many different forms and that discovers which form
               is best for a given dataset. The model makes probabilistic
               inferences over a space of graph grammars representing trees,
               linear orders, multidimensional spaces, rings, dominance
               hierarchies, cliques, and other forms and successfully discovers
               the underlying structure of a variety of physical, biological,
               and social domains. Our approach brings structure learning
               methods closer to human abilities and may lead to a deeper
               computational understanding of cognitive development.",
  journal   = "Proceedings of the National Academy of Sciences",
  publisher = "National Acad Sciences",
  volume    =  105,
  number    =  31,
  pages     = "10687--10692",
  month     =  aug,
  year      =  2008,
  keywords  = "Algorithms; Data Interpretation; Humans; Learning; Learning:
               physiology; Models; Research Design; Statistical; Theoretical"
}
@BOOK{Box1976-qk,
  title  = "Time series analysis: forecasting and control",
  author = "Box, G E P and Jenkins, G M and Reinsel, G C",
  year   =  1976
}
@INPROCEEDINGS{Lloyd2014-nz,
  title         = "Automatic Construction and {Natural-Language} Description of
                   Nonparametric Regression Models",
  booktitle     = "Association for the Advancement of Artificial Intelligence
                   ({AAAI})",
  author        = "Lloyd, James Robert and Duvenaud, David and Grosse, Roger
                   and Tenenbaum, Joshua B and Ghahramani, Zoubin",
  abstract      = "This paper presents the beginnings of an automatic
                   statistician, focusing on regression problems. Our system
                   explores an open-ended space of possible statistical models
                   to discover a good explanation of the data, and then
                   produces a detailed report with figures and natural-language
                   text. Our approach treats unknown functions
                   nonparametrically using Gaussian processes, which has two
                   important consequences. First, Gaussian processes model
                   functions in terms of high-level properties (e.g.
                   smoothness, trends, periodicity, changepoints). Taken
                   together with the compositional structure of our language of
                   models, this allows us to automatically describe functions
                   through a decomposition into additive parts. Second, the use
                   of flexible nonparametric models and a rich language for
                   composing them in an open-ended manner also results in
                   state-of-the-art extrapolation performance evaluated over 13
                   real time series data sets from various domains.",
  month         =  jul,
  year          =  2014,
  conference    = "Association for the Advancement of Artificial Intelligence
                   (AAAI)",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML"
}
@ARTICLE{Lean1995-vp,
  title     = "Reconstruction of solar irradiance since 1610: Implications for
               climate change",
  author    = "Lean, J and Beer, J and Bradley, R",
  journal   = "Geophys. Res. Lett.",
  publisher = "AGU AMERICAN GEOPHYSICAL UNION",
  volume    =  22,
  number    =  23,
  pages     = "3195--3198",
  year      =  1995
}
@ARTICLE{Barbu2012-wv,
  title    = "An Introduction to Artificial Prediction Markets for
              Classification",
  author   = "Barbu, Adrian and Lay, Nathan",
  journal  = "J. Mach. Learn. Res.",
  volume   =  13,
  pages    = "2177--2204",
  year     =  2012,
  keywords = "ensemble methods; implicit on-; online learning; random forest;
              supervised learning"
}
@ARTICLE{Ganesalingam_undated-us,
  title   = "A fully automatic problem solver with human-style output",
  author  = "Ganesalingam, M and Gowers, W T",
  journal = "arXiv preprint 1309.4501",
  year    =  2013
}
@ARTICLE{Airoldi2008-fr,
  title    = "Mixed Membership Stochastic Blockmodels",
  author   = "Airoldi, Edoardo M. and Blei, David M. and Fienberg, Stephen E.
              and Xing, Eric P.",
  abstract = "Observations consisting of measurements on relationships for
              pairs of objects arise in many settings, such as protein
              interaction and gene regulatory networks, collections of
              author-recipient email, and social networks. Analyzing such data
              with probabilisic models can be delicate because the simple
              exchangeability assumptions underlying many boilerplate models no
              longer hold. In this paper, we describe a latent variable model
              of such data called the mixed membership stochastic blockmodel.
              This model extends blockmodels for relational data to ones which
              capture mixed membership latent relational structure, thus
              providing an object-specific low-dimensional representation. We
              develop a general variational inference algorithm for fast
              approximate posterior inference. We explore applications to
              social and protein interaction networks.",
  journal  = "J. Mach. Learn. Res.",
  volume   =  9,
  pages    = "1981--2014",
  month    =  sep,
  year     =  2008,
  keywords = "analysis; hierarchical bayes; latent variables; mean-field
              approximation; protein interaction networks; social networks;
              statistical network"
}
@ARTICLE{Hoff2002-vy,
  title    = "Latent Space Approaches to Social Network Analysis",
  author   = "Hoff, Peter D. and Raftery, Adrian E and Handcock, Mark S",
  journal  = "J. Am. Stat. Assoc.",
  volume   =  97,
  number   =  460,
  pages    = "1090--1098",
  month    =  dec,
  year     =  2002,
  keywords = "conditional independence model; latent position model; network
              data; random graph; visualization"
}
@INPROCEEDINGS{Xu2012-ub,
  title     = "Infinite Tucker Decomposition: Nonparametric Bayesian Models for
               Multiway Data Analysis",
  booktitle = "Proceedings of the International Conference on Machine Learning
               ({ICML})",
  author    = "Xu, Zenglin and Yan, Feng and Qi, Yuan",
  year      =  2012
}
@INPROCEEDINGS{Salakhutdinov2008-tp,
  title     = "Probabilistic matrix factorization",
  booktitle = "Advances in Neural Information Processing Systems ({NIPS})",
  author    = "Salakhutdinov, Ruslan and Mnih, Andriy",
  pages     = "1--8",
  year      =  2008
}
@ARTICLE{Lovasz2006-hc,
  title    = "Limits of dense graph sequences",
  author   = "Lovasz, Laszlo and Szegedy, Balazs",
  abstract = "We show that if a sequence of dense graphs has the property that
              for every fixed graph F, the density of copies of F in these
              graphs tends to a limit, then there is a natural ``limit
              object'', namely a symmetric measurable 2-variable function on
              [0,1]. This limit object determines all the limits of subgraph
              densities. We also show that the graph parameters obtained as
              limits of subgraph densities can be characterized by ``reflection
              positivity'', semidefiniteness of an associated matrix.
              Conversely, every such function arises as a limit object. Along
              the lines we introduce a rather general model of random graphs,
              which seems to be interesting on its own right.",
  journal  = "Journal of Combinatorial Theory Series B,",
  year     =  2006
}
@BOOK{Jaynes2003-jh,
  title     = "Probability theory: The logic of science",
  author    = "Jaynes, Edwin T",
  publisher = "Cambridge University Press",
  year      =  2003
}
@ARTICLE{Kallenberg1999-pj,
  title    = "Multivariate Sampling and the Estimation Problem for Exchangeable
              Arrays",
  author   = "Kallenberg, Olav",
  journal  = "Journal of Theoretical Probability",
  volume   =  12,
  number   =  3,
  pages    = "859--883",
  year     =  1999,
  keywords = "consistent estimation; empirical distributions; exchangeable
              arrays; functional represen-; limit theorems; stationary
              processes; tations"
}
@INPROCEEDINGS{Lloyd_undated-iu,
  title     = "Exchangeable databases and their functional representation",
  booktitle = "Frontiers of Network Analysis: Methods, Models, and Applications
               at {NIPS}",
  author    = "Lloyd, J R and Orbanz, P and Ghahramani, Z and Roy, D M",
  abstract  = "We consider the task of statistical inference for data in the
               form of a relational database comprising multiple relations
               acting on heterogenous sets of objects. We define a notion of
               exchangeability for databases generalizing that of arrays, based
               on the idea that ...",
  year      =  2013
}
@PHDTHESIS{Duvenaud2014-em,
  title  = "Automatic Model Construction with Gaussian Processes",
  author = "Duvenaud, David",
  year   =  2014,
  school = "University of Cambridge"
}
@INPROCEEDINGS{Lloyd2013-yn,
  title     = "{GEFCom2012} Hierarchical Load Forecasting: Gradient Boosting Machines and Gaussian Processes",
  booktitle = "International Journal of Forecasting",
  author    = "Lloyd, James Robert",
  year      = 2013
}
@ARTICLE{Cox1946-qc,
  title   = "Probability, Frequency and Reasonable Expectation",
  author  = "Cox, Richard Threlkeld",
  journal = "Am. J. Phys.",
  volume  =  14,
  pages   = "1--13",
  month   =  jan,
  year    =  1946
}
@BOOK{Kallenberg2002-il,
  title     = "Foundations of Modern Probability",
  author    = "Kallenberg, Olav",
  publisher = "Springer",
  year      =  2002
}
@INPROCEEDINGS{Lawrence2009-za,
  title     = "Non-linear matrix factorization with Gaussian processes",
  booktitle = "Proceedings of the International Conference on Machine Learning
               ({ICML})",
  author    = "Lawrence, Neil D. and Urtasun, Raquel",
  publisher = "ACM Press",
  pages     = "1--8",
  year      =  2009
}
@ARTICLE{Aldous1981-lg,
  title   = "Representations for partially exchangeable arrays of random
             variables",
  author  = "Aldous, David J.",
  journal = "J. Multivar. Anal.",
  volume  =  11,
  number  =  4,
  pages   = "581--598",
  year    =  1981
}
@TECHREPORT{Hoover1979-br,
  title       = "Relations on Probability Spaces and Arrays of Random Variables",
  author      = "Hoover, Douglas N",
  institution = "Institute for Advanced Study, Princeton",
  year        =  1979
}
@INPROCEEDINGS{Kemp2006-jt,
  title     = "Learning systems of concepts with an infinite relational model",
  booktitle = "Proceedings of the National Conference on Artificial
               Intelligence",
  author    = "Kemp, Charles and Tenenbaum, J B and Griffiths, T L and Yamada,
               T and Ueda, N",
  volume    =  21,
  year      =  2006
}
@ARTICLE{Miller2009-wg,
  title   = "Nonparametric latent feature models for link prediction",
  author  = "Miller, Kurt T. and Griffiths, Thomas L. and Jordan, Michael I.",
  journal = "Adv. Neural Inf. Process. Syst.",
  pages   = "1276--1284",
  year    =  2009
}
@INPROCEEDINGS{Palla2012-ch,
  title     = "An Infinite Latent Attribute Model for Network Data",
  booktitle = "Proceedings of the International Conference on Machine Learning
               ({ICML})",
  author    = "Palla, Konstantina and Knowles, David A and Ghahramani, Zoubin",
  year      =  2012
}
@INPROCEEDINGS{Xu2006-uy,
  title     = "Infinite hidden relational models",
  booktitle = "Proceedings of the Conference on Uncertainty in Artificial
               Intelligence ({UAI})",
  author    = "Xu, Zhao and Tresp, Volker and Yu, Kai",
  year      =  2006
}
@INPROCEEDINGS{Meeds2007-gd,
  title     = "Modeling dyadic data with binary latent factors",
  booktitle = "Advances in Neural Information Processing Systems ({NIPS})",
  author    = "Meeds, Edward and Ghahramani, Zoubin and Neal, Radford M. and
               Roweis, Sam T.",
  publisher = "Citeseer",
  year      =  2007
}
@INPROCEEDINGS{Yan2011-lc,
  title     = "Sparse matrix-variate Gaussian process blockmodels for network
               modeling",
  booktitle = "Proceedings of the International Conference on Uncertainty in
               Artificial Intelligence ({UAI})",
  author    = "Yan, Feng and Xu, Zenglin and Qi, Yuan",
  year      =  2011
}
@PHDTHESIS{Saatci2011-yo,
  title  = "Scalable Inference for Structured Gaussian Process Models",
  author = "Saat\c{c}i, Yunus",
  year   =  2011,
  school = "University of Cambridge"
}
@ARTICLE{Neal2003-zv,
  title     = "Slice Sampling",
  author    = "Neal, Radford M",
  abstract  = "Markov chain sampling methods that adapt to characteristics of
               the distribution being sampled can be constructed using the
               principle that one can sample from a distribution by sampling
               uniformly from the region under the plot of its density
               function. A Markov chain that converges to this uniform
               distribution can be constructed by alternating uniform sampling
               in the vertical direction with uniform sampling from the
               horizontal ``slice'' defined by the current vertical position,
               or more generally, with some update that leaves the uniform
               distribution over this slice invariant. Such ``slice sampling''
               methods are easily implemented for univariate distributions, and
               can be used to sample from a multivariate distribution by
               updating each variable in turn. This approach is often easier to
               implement than Gibbs sampling and more efficient than simple
               Metropolis updates, due to the ability of slice sampling to
               adaptively choose the magnitude of changes made. It is therefore
               attractive for routine and automated use. Slice sampling methods
               that update all variables simultaneously are also possible.
               These methods can adaptively choose the magnitudes of changes
               made to each variable, based on the local properties of the
               density function. More ambitiously, such methods could
               potentially adapt to the dependencies between variables by
               constructing local quadratic approximations. Another approach is
               to improve sampling efficiency by suppressing random walks. This
               can be done for univariate slice sampling by ``overrelaxation,''
               and for multivariate slice sampling by ``reflection'' from the
               edges of the slice.",
  journal   = "Ann. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  31,
  number    =  3,
  pages     = "705--741",
  month     =  "1~" # jun,
  year      =  2003
}
@INPROCEEDINGS{Alex_J_Smola2001-ev,
  title     = "Sparse Greedy Gaussian Process Regression",
  booktitle = "Advances in Neural Information Processing Systems 13",
  author    = "Smola, Alex J and Bartlett, Peter",
  abstract  = "CiteSeerX - Document Details (Isaac Councill, Lee Giles, Pradeep
               Teregowda): We present a simple sparse greedy technique to
               approximate the maximum a posteriori estimate of Gaussian
               Processes with much improved scaling behaviour in the sample
               size m. In particular, computational requirements are O(n m),
               storage is O(nm), the cost for prediction is O(n) and the cost
               to compute confidence bounds is O(nm), where n m. We show how to
               compute a stopping criterion, give bounds on the approximation
               error, and show applications to large scale problems.",
  year      =  2001
}
@INCOLLECTION{Wahba1999-bl,
  title     = "The {Bias-Variance} Tradeoff and the Randomized {GACV}",
  booktitle = "Advances in Neural Information Processing Systems 11",
  author    = "Wahba, Grace and Lin, Xiwu and Gao, Fangyu and Xiang, Dong and
               Klein, Ronald and Klein, Barbara",
  editor    = "Kearns, M J and Solla, S A and Cohn, D A",
  publisher = "MIT Press",
  pages     = "620--626",
  year      =  1999
}
@ARTICLE{Silverman1985-ys,
  title     = "Some Aspects of the Spline Smoothing Approach to
               {Non-Parametric} Regression Curve Fitting",
  author    = "Silverman, B W",
  abstract  = "Non-parametric regression using cubic splines is an attractive,
               flexible and widely-applicable approach to curve estimation.
               Although the basic idea was formulated many years ago, the
               method is not as widely known or adopted as perhaps it should
               be. The topics and examples discussed in this paper are intended
               to promote the understanding and extend the practicability of
               the spline smoothing methodology. Particular subjects covered
               include the basic principles of the method; the relation with
               moving average and other smoothing methods; the automatic choice
               of the amount of smoothing; and the use of residuals for
               diagnostic checking and model adaptation. The question of
               providing inference regions for curves-and for relevant
               properties of curves--is approached via a finite-dimensional
               Bayesian formulation.",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "Wiley for the Royal Statistical Society",
  volume    =  47,
  number    =  1,
  pages     = "1--52",
  month     =  "1~" # jan,
  year      =  1985,
  issn_alt  = "0035-9246"
}
@INPROCEEDINGS{Titsias2008-gp,
  title     = "Efficient sampling for Gaussian process inference using control
               variables",
  booktitle = "Advances in Neural Information Processing Systems ({NIPS})",
  author    = "Titsias, Michalis K. and Lawrence, Neil D.",
  pages     = "1681--1688",
  year      =  2008
}
@ARTICLE{Murray2010-zu,
  title   = "Elliptical slice sampling",
  author  = "Murray, Iain and Adams, Ryan Prescott and Mackay, David J C",
  journal = "J. Mach. Learn. Res.",
  volume  =  9,
  pages   = "541--548",
  year    =  2010
}
@ARTICLE{Kallenberg1992-gb,
  title    = "Symmetries on random arrays and set-indexed processes",
  author   = "Kallenberg, Olav",
  journal  = "J. Theoret. Probab.",
  volume   =  5,
  number   =  4,
  pages    = "727--765",
  month    =  oct,
  year     =  1992,
  issn_alt = "1572-9230"
}
@INPROCEEDINGS{Hoover1982-ty,
  title     = "Row-column exchangeability and a generalized model for
               probability",
  booktitle = "Exchangeability in Probability and Statistics",
  author    = "Hoover, D N",
  pages     = "281--291",
  year      =  1982
}
@ARTICLE{Diaconis2007-mi,
  title   = "Graph limits and exchangeable random graphs",
  author  = "Diaconis, Persi and Janson, Svante",
  journal = "arXiv preprint arXiv:0712.2749",
  pages   = "1--26",
  year    =  2007
}
@INPROCEEDINGS{Aldous2010-iw,
  title     = "More uses of exchangeability: Representations of complex random
               structures",
  booktitle = "Probability and Mathematical Genetics: Papers in Honour of Sir
               John Kingman",
  author    = "Aldous, David J.",
  year      =  2010
}
@TECHREPORT{Austin2012-jq,
  title  = "Exchangeable random arrays",
  author = "Austin, Tim",
  year   =  2012
}
@ARTICLE{Choi2013-th,
  title    = "Co-clustering separately exchangeable network data",
  author   = "Choi, David S. and Wolfe, Patrick J.",
  abstract = "This article establishes the performance of stochastic
              blockmodels in addressing the co-clustering problem of
              partitioning a binary array into subsets, assuming only that the
              data are generated by a nonparametric process satisfying the
              condition of separate exchangeability. We provide oracle
              inequalities with rate of convergence O\_P(n^(-1/4))
              corresponding to profile likelihood maximization and mean-square
              error minimization, and show that the blockmodel can be
              interpreted in this setting as an optimal piecewise-constant
              approximation to the generative nonparametric model. We also show
              for large sample sizes that the detection of co-clusters in such
              data indicates with high probability the existence of co-clusters
              of equal size and asymptotically equivalent connectivity in the
              underlying generative process.",
  journal  = "Ann. Stat.",
  month    =  dec,
  year     =  2013,
  keywords = "05C80; 60B20; 62G05; bipartite graph; network clustering; oracle
              inequality; profile likelihood; statistical network analysis;
              stochastic blockmodel and co-blockmodel"
}
@ARTICLE{Wolfe2013-vs,
  title    = "Nonparametric graphon estimation",
  author   = "Wolfe, Patrick J. and Olhede, Sofia C.",
  journal  = "arXiv preprint arXiv:1309.5936",
  pages    = "1--52",
  year     =  2013,
  keywords = "05C80; 62G05; 62G20; and phrases; graph limits; nonparametric
              regression; sparse random graphs; statistical network analysis;
              stochastic blockmodels"
}
@BOOK{Ullman2002-eo,
  title     = "A First Course in Database Systems",
  author    = "Ullman, J and Widom, J",
  publisher = "Prentice Hall",
  year      =  2002
}
@INPROCEEDINGS{Yu2008-tz,
  title     = "Gaussian process models for link analysis and transfer learning",
  booktitle = "Advances in Neural Information Processing Systems ({NIPS})",
  author    = "Yu, Kai and Chu, Wei",
  year      =  2008
}
@ARTICLE{Wang1987-jd,
  title    = "Stochastic Blockmodels for Directed Graphs",
  author   = "Wang, Yuchung J. and Wong, George Y.",
  journal  = "J. Am. Stat. Assoc.",
  volume   =  82,
  number   =  397,
  pages    = "8--19",
  year     =  1987,
  keywords = "blockmodeling; iterative scaling algorithm; stochastic
              partitioned directed graph; technique"
}
@ARTICLE{Nowicki2001-xm,
  title    = "Estimation and Prediction for Stochastic Blockstructures",
  author   = "Nowicki, Krzysztof and Snijders, Tom A B",
  journal  = "J. Am. Stat. Assoc.",
  volume   =  96,
  number   =  455,
  pages    = "1077--1087",
  year     =  2001,
  keywords = "cluster analysis; colored graph; gibbs sampling; latent class
              model; mixture model; social network"
}
@ARTICLE{Acar2013-na,
  title     = "Understanding Data Fusion Within the Framework of Coupled Matrix
               and Tensor Factorizations",
  author    = "Acar, Evrim and Rasmussen, Morten Arendt and Savorani, Francesco
               and N\ae{}s, Tormod and Bro, Rasmus",
  journal   = "Chemometrics Intellig. Lab. Syst.",
  publisher = "Elsevier B.V.",
  month     =  jun,
  year      =  2013,
  keywords  = "candecomp; data; data fusion; matrix factorization; missing;
               parafac; tensor factorization"
}
@ARTICLE{Acar2012-no,
  title    = "Coupled analysis of in vitro and histology tissue samples to
              quantify structure-function relationship",
  author   = "Acar, Evrim and Plopper, George E. and Yener, B{\"{u}}lent",
  abstract = "The structure/function relationship is fundamental to our
              understanding of biological systems at all levels, and drives
              most, if not all, techniques for detecting, diagnosing, and
              treating disease. However, at the tissue level of biological
              complexity we encounter a gap in the structure/function
              relationship: having accumulated an extraordinary amount of
              detailed information about biological tissues at the cellular and
              subcellular level, we cannot assemble it in a way that explains
              the correspondingly complex biological functions these structures
              perform. To help close this information gap we define here
              several quantitative temperospatial features that link tissue
              structure to its corresponding biological function. Both
              histological images of human tissue samples and fluorescence
              images of three-dimensional cultures of human cells are used to
              compare the accuracy of in vitro culture models with their
              corresponding human tissues. To the best of our knowledge, there
              is no prior work on a quantitative comparison of histology and in
              vitro samples. Features are calculated from graph theoretical
              representations of tissue structures and the data are analyzed in
              the form of matrices and higher-order tensors using matrix and
              tensor factorization methods, with a goal of differentiating
              between cancerous and healthy states of brain, breast, and bone
              tissues. We also show that our techniques can differentiate
              between the structural organization of native tissues and their
              corresponding in vitro engineered cell culture models.",
  journal  = "PLoS One",
  volume   =  7,
  number   =  3,
  pages    = "e32227",
  month    =  jan,
  year     =  2012,
  keywords = "Algorithms; Bone and Bones; Bone and Bones: anatomy \& histology;
              Bone and Bones: cytology; Bone Neoplasms; Bone Neoplasms:
              pathology; Brain; Brain: anatomy \& histology; Brain: cytology;
              Breast; Breast Neoplasms; Breast Neoplasms: pathology; Breast:
              anatomy \& histology; Breast: cytology; Cell Culture Techniques;
              Female; Glioma; Glioma: pathology; Humans; Image Interpretation,
              Computer-Assisted; Image Interpretation, Computer-Assisted:
              methods; Imaging, Three-Dimensional; Imaging, Three-Dimensional:
              methods; Models, Anatomic"
}
@INPROCEEDINGS{Acar2011-vg,
  title     = "All-at-once Optimization for Coupled Matrix and Tensor
               Factorizations",
  booktitle = "Ninth workshop on mining and learning with graphs",
  author    = "Acar, Evrim and Kolda, Tamara G and Dunlavy, Daniel M",
  abstract  = "Array",
  year      =  2011,
  keywords  = "can-; data fusion; decomp; matrix factorizations; missing data;
               parafac; tensor factorizations"
}
@ARTICLE{Andersen2013-rg,
  title    = "Complex {Multi-Block} Analysis Identifies New Immunologic and
              Genetic Disease Progression Patterns Associated with the Residual
              $\beta$-Cell Function 1 Year after Diagnosis of Type 1 Diabetes",
  author   = "Andersen, Marie Louise Max and Rasmussen, Morten Arendt and
              P{\"{o}}rksen, Sven and Svensson, Jannet and Vikre-J\o{}rgensen,
              Jennifer and Thomsen, Jane and Hertel, Niels Thomas and
              Johannesen, Jesper and Pociot, Flemming and Petersen, Jacob Sten
              and Hansen, Lars and Mortensen, Henrik Bindesb\o{}l and Nielsen,
              Lotte Br\o{}ndum",
  abstract = "The purpose of the present study is to explore the progression of
              type 1 diabetes (T1D) in Danish children 12 months after
              diagnosis using Latent Factor Modelling. We include three data
              blocks of dynamic paraclinical biomarkers, baseline clinical
              characteristics and genetic profiles of diabetes related SNPs in
              the analyses. This method identified a model explaining 21.6\% of
              the total variation in the data set. The model consists of two
              components: (1) A pattern of declining residual $\beta$-cell
              function positively associated with young age, presence of
              diabetic ketoacidosis and long duration of disease symptoms (P =
              0.0004), and with risk alleles of WFS1, CDKN2A/2B and RNLS (P =
              0.006). (2) A second pattern of high ZnT8 autoantibody levels and
              low postprandial glucagon levels associated with risk alleles of
              IFIH1, TCF2, TAF5L, IL2RA and PTPN2 and protective alleles of
              ERBB3 gene (P = 0.0005). These results demonstrate that Latent
              Factor Modelling can identify associating patterns in clinical
              prospective data - future functional studies will be needed to
              clarify the relevance of these patterns.",
  journal  = "PLoS One",
  volume   =  8,
  number   =  6,
  pages    = "e64632",
  month    =  jan,
  year     =  2013
}
@INPROCEEDINGS{Yin2013-we,
  title     = "Connecting Comments and Tags : Improved Modeling of Social
               Tagging Systems Categories and Subject Descriptors",
  booktitle = "Proceedings of the sixth {ACM} international conference on Web
               search and data mining",
  author    = "Yin, Dawei and Guo, Shengbo and Chidlovskii, Boris and Davison,
               Brian D and Archambeau, Cedric and Bouchard, Guillaume",
  pages     = "547--556",
  year      =  2013,
  keywords  = "co-; personalized comment recommendation; personalized tag;
               prediction; social tagging; tag recommendation"
}
@ARTICLE{Ermis2012-gk,
  title    = "Link Prediction via Generalized Coupled Tensor Factorisation",
  author   = "Ermis, Beyza and Acar, Evrim and Cemgil, A. Taylan",
  journal  = "arXiv preprint arXiv:1208.6231",
  year     =  2012,
  keywords = "coupled tensor factorisation; link prediction; missing data"
}
@INPROCEEDINGS{Gao2011-ac,
  title     = "Link Pattern Prediction with Tensor Decomposition in
               Multi-relational Networks",
  booktitle = "2011 {IEEE} Symposium on Computational Intelligence and Data
               Mining ({CIDM})",
  author    = "Gao, Sheng and Denoyer, Ludovic and Gallinari, Patrick",
  year      =  2011
}
@ARTICLE{Jimeng2009-rw,
  title    = "{MetaFac} : Community Discovery via Relational Hypergraph
              Factorization",
  author   = "Jimeng, Yu-Ru Lin and Paul, Sun and Ravi, Castro and Hari, Konuru
              and Aisling, Sundaram",
  pages    = "527--535",
  year     =  2009,
  keywords = "community discovery; dynamic; metafac; metagraph factorization;
              negative tensor factorization; non-; relational hypergraph;
              social network analysis"
}
@ARTICLE{Lippert2008-gg,
  title    = "Relation Prediction in {Multi-Relational} Domains using Matrix
              Factorization",
  author   = "Lippert, Christoph and Weber, Stefan Hagen and Huang, Yi and
              Tresp, Volker and Schubert, Matthias and Kriegel, Hans-Peter",
  number   = "Siso",
  year     =  2008,
  keywords = "bioinformatics; collaborative filtering; matrix factorization;
              relation prediction; relational-learning"
}
@INPROCEEDINGS{Nickel2011-pi,
  title     = "A {Three-Way} Model for Collective Learning on
               {Multi-Relational} Data",
  booktitle = "Proceedings of the 28th international conference on machine
               learning ({ICML-11})",
  author    = "Nickel, Maximilian",
  year      =  2011
}
@INPROCEEDINGS{Shangguan2012-ga,
  title     = "Book Recommendation Based on Joint Multi-relational Model",
  booktitle = "2012 Second International Conference on Cloud and Green
               Computing",
  author    = "Shangguan, Qiuzi and Hu, Liang and Cao, Jian and Xu, Guandong",
  publisher = "Ieee",
  pages     = "523--530",
  month     =  nov,
  year      =  2012,
  keywords  = "-recommendation; coupled matrix; factorization; genetic
               algorithm; mult-relational model; social network"
}
@INPROCEEDINGS{Singh2008-cb,
  title     = "Relational Learning via Collective Matrix Factorization
               Categories and Subject Descriptors",
  booktitle = "Proceedings of the 14th {ACM} {SIGKDD} international conference
               on Knowledge discovery and data mining",
  author    = "Singh, Ajit P and Gordon, Geoffrey J",
  pages     = "650--658",
  year      =  2008
}
@INPROCEEDINGS{Singh2008-qw,
  title     = "A Unified View of Matrix Factorization Models",
  booktitle = "Machine Learning and Knowledge Discovery in Databases",
  author    = "Singh, Ajit P and Gordon, Geoffrey J",
  pages     = "358--373",
  year      =  2008
}
@ARTICLE{Singh2012-jj,
  title   = "A Bayesian Matrix Factorization Model for Relational Data",
  author  = "Singh, Ajit P and Gordon, Geoffrey J",
  journal = "arXiv preprint arXiv:1203.3517",
  year    =  2012
}
@BOOK{MacKay2003-rp,
  title     = "Information Theory, Inference, and Learning Algorithms",
  author    = "MacKay, David J",
  publisher = "Cambridge University Press",
  year      =  2003
}
@ARTICLE{Lazaro-gredilla2010-hc,
  title    = "Sparse Spectrum Gaussian Process Regression",
  author   = "L\'{a}zar{o-G}redilla, Miguel and Qui\~{n}oner{o-C}andela, Joaquin",
  volume   =  11,
  pages    = "1865--1881",
  year     =  2010,
  keywords = "computational efficiency; gaussian process; power spectrum;
              probabilistic regression; sparse approximation"
}
@BOOK{Bochner1959-yk,
  title     = "Lectures on Fourier Integrals",
  author    = "Bochner, S and Tenenbaum, M and Pollard, H",
  publisher = "Princeton University Press",
  series    = "Annals of mathematics studies",
  year      =  1959
}
@INPROCEEDINGS{Klenske_undated-ys,
  title     = "Nonparametric dynamics estimation for time periodic systems",
  booktitle = "2013 51st Annual Allerton Conference on Communication, Control,
               and Computing (Allerton)",
  author    = "Klenske, Edgar D and Zeilinger, Melanie N and Sch{\"{o}}lkopf,
               Bernhard and Hennig, Philipp",
  publisher = "IEEE",
  pages     = "486--493",
  year      = 2013,
  isbn_alt  = "9781479934096"
}
@ARTICLE{Kronberger_undated-vf,
  title    = "Evolution of Covariance Functions for Gaussian Process Regression
              using Genetic Programming",
  author   = "Kronberger, Gabriel and Kommenda, Michael",
  journal  = "arXiv:1305.3794",
  year     =  2013,
  keywords = "gaussian process; genetic programming; structure identification"
}
@MISC{Nutonian2011-el,
  title  = "Eureqa",
  author = "{Nutonian}",
  year   =  2011
}
@INPROCEEDINGS{Bach2004-lb,
  title     = "Multiple kernel learning, conic duality, and the {SMO} algorithm",
  booktitle = "Proceedings of the twenty-first international conference on
               Machine learning",
  author    = "Bach, Francis R and Lanckriet, Gert R G and Jordan, Michael I",
  publisher = "ACM",
  pages     = "6",
  month     =  "4~" # jul,
  year      =  2004
}
@ARTICLE{Garnett2010-rd,
  title    = "Sequential Bayesian Prediction in the Presence of Changepoints
              and Faults",
  author   = "Garnett, R and Osborne, M. a. and Reece, S and Rogers, a. and
              Roberts, S J",
  journal  = "Comput. J.",
  volume   =  53,
  number   =  9,
  pages    = "1430--1446",
  month    =  feb,
  year     =  2010,
  keywords = "changepoint detection; fault detection; gaussian processes;
              time-series prediction"
}
@INPROCEEDINGS{Saatci2010-el,
  title     = "Gaussian process change point models",
  booktitle = "International Conference on Machine Learning",
  author    = "Saat\c{c}i, Yunis and Turner, Richard D and Rasmussen, Carl E",
  abstract  = "Abstract We combine Bayesian online change point detection with
               Gaussian processes to create a nonparametric time series model
               which can handle change points. The model can be used to locate
               change points in an online manner; and, unlike other Bayesian
               online ...",
  publisher = "machinelearning.wustl.edu",
  year      =  2010
}
@INPROCEEDINGS{Fox2012-fm,
  title     = "Multiresolution Gaussian Processes",
  booktitle = "Advances in Neural Information Processing Systems",
  author    = "Fox, Emily B and Dunson, David B",
  pages     = "737--745",
  year      =  2012
}
@ARTICLE{Lind2006-th,
  title     = "Basic statistics for business and economics",
  author    = "Lind, D A and Marchal, W G and Wathen, S A and Magazine, B W",
  abstract  = "CONTENTS CHAPTER 1 Introduction to Basic Statistics 1 Goals 1
               What Is Statistics ? 2 A Statistic and Statistics 3 Who Uses
               Statistics ? 3 Misuses of Statistics 5 Types of Statistics 6
               Descriptive Statistics 6 inferential Statistics 7 Use of
               Questionnaires 9 ComputerApplications 10 Why ...",
  publisher = "dijlahlibrary.com",
  year      =  2006
}
@MISC{Hyndman_undated-zj,
  title        = "Time Series Data Library",
  author       = "Hyndman, Rob J",
  howpublished = "\url{http://data.is/TSDLdemo}",
  note         = "Accessed: 2013"
}
@MISC{Stan_Development_Team2014-ha,
  title  = "Stan: A {C++} Library for Probability and Sampling, Version 2.2",
  author = "{Stan Development Team}",
  year   =  2014
}

@BOOK{Popper2005-qq,
  title     = "The logic of scientific discovery",
  author    = "Popper, K",
  abstract  = "Described by the philosopher AJ Ayer as a work of'great
               originality and power', this book revolutionized contemporary
               thinking on science and knowledge. Ideas such as the now
               legendary doctrine of'falsificationism'electrified the
               scientific community, influencing even ...",
  publisher = "Routledge",
  year      =  2005
}

@ARTICLE{Koller1997-am,
  title     = "Effective Bayesian inference for stochastic programs",
  author    = "Koller, D and McAllester, D and Pfeffer, A",
  abstract  = "Abstract In this paper, we propose a stochastic version of a
               general purpose functional programming language as a method of
               modeling stochastic processes. The language contains random
               choices, conditional statements, structured values, defined
               functions, ...",
  journal   = "Association for the Advancement of Artificial Intelligence
               (AAAI)",
  publisher = "aaai.org",
  year      =  1997
}

@ARTICLE{Gelman2003-xx,
  title     = "A {Bayesian} Formulation of Exploratory Data Analysis and
               {goodness-of-fit} Testing",
  author    = "Gelman, Andrew",
  abstract  = "R\'{e}sum\'{e} Analyse de donn\'{e}es exploratrices et
               inf\'{e}rence (EDA) Bay\'{e}sienne (ou, en large,
               mod\'{e}lisation de statistiques complexes)-qui sont
               g\'{e}n\'{e}ralement consid\'{e}r\'{e}es comme \'{e}tant des
               paradigmes statistiques non relies. Dans cet article, nous
               pr\'{e}sentons un cadre pour l' ...",
  journal   = "Int. Stat. Rev.",
  publisher = "Wiley Online Library",
  year      =  2003
}

@ARTICLE{Cowles1996-qy,
  title     = "Markov Chain Monte Carlo Convergence Diagnostics: A Comparative
               Review",
  author    = "Cowles, Mary Kathryn and Carlin, Bradley P",
  abstract  = "A critical issue for users of Markov chain Monte Carlo (MCMC)
               methods in applications is how to determine when it is safe to
               stop sampling and use the samples to estimate characteristics of
               the distribution of interest. Research into methods of computing
               theoretical convergence bounds holds promise for the future but
               to date has yielded relatively little of practical use in
               applied work. Consequently, most MCMC users address the
               convergence problem by applying diagnostic tools to the output
               produced by running their samplers. After giving a brief
               overview of the area, we provide an expository review of 13
               convergence diagnostics, describing the theoretical basis and
               practical implementation of each. We then compare their
               performance in two simple models and conclude that all of the
               methods can fail to detect the sorts of convergence failure that
               they were designed to identify. We thus recommend a combination
               of strategies aimed at evaluating and accelerating MCMC sampler
               convergence, including applying diagnostic procedures to a small
               number of parallel chains, monitoring autocorrelations and
               cross-correlations, and modifying parametrizations or sampling
               algorithms appropriately. We emphasize, however, that it is not
               possible to say with certainty that a finite sample from an MCMC
               algorithm is representative of an underlying stationary
               distribution.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "American Statistical Association",
  volume    =  91,
  number    =  434,
  pages     = "883--904",
  month     =  "1~" # jun,
  year      =  1996
}

@ARTICLE{Benjamini_undated-mh,
  title    = "Controlling the False Discovery Rate: A Practical and Powerful
              Approach to Multiple Testing",
  author   = "Benjamini, Yoav and Hochberg, Yosef",
  journal  = "J. R. Stat. Soc. Series B Stat. Methodol."
}

@MISC{deep-learning-tutorial,
  author   = "{Deep learning tutorial authors}",
  title    = "Deep learning tutorial - http://www.deeplearning.net/tutorial/",
  year     =  2014
}

@ARTICLE{Parzen1962-hk,
  title     = "On estimation of a probability density function and mode",
  author    = "Parzen, E",
  abstract  = "The problem of estimation of a probability density function f
               (x) is interesting, for many reasons. As one possible
               application, we mention the problem of estimating the hazard, or
               conditional rate of failure, function /(ж)/1-F (x), where F (x)
               is the distribution function ...",
  journal   = "Ann. Math. Stat.",
  publisher = "ssg.mit.edu",
  year      =  1962
}

@ARTICLE{Rosenblatt1956-hx,
  title     = "Remarks on Some Nonparametric Estimates of a Density Function",
  author    = "Rosenblatt, Murray",
  abstract  = "Project Euclid - mathematics and statistics online",
  journal   = "Ann. Math. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  27,
  number    =  3,
  pages     = "832--837",
  month     =  sep,
  year      =  1956,
  issn_alt  = "2168-8990"
}

@ARTICLE{Guttman1967-my,
  title     = "The Use of the Concept of a Future Observation in
               {goodness-of-fit} Problems",
  author    = "Guttman, Irwin",
  abstract  = "An attack on the problem of goodness of fit is made by combining
               a Bayesian and sampling argument; the Bayesian part is effected
               by using the distribution of a future observation, while the
               sampling argument concerns itself with the distribution of a
               {"}chi-squared like{"} statistic, which measures discrepancies
               of observed frequencies from those predicted by the distribution
               of the future observation. Examples are given for the case of
               sampling from the binomial, Poisson and normal distributions. An
               interesting application arising from the above approach is a
               procedure for estimating the degree of a polynomial response
               function.",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "Wiley for the Royal Statistical Society",
  volume    =  29,
  number    =  1,
  pages     = "83--100",
  year      =  1967
}

@ARTICLE{Bayarri2007-cp,
  title     = "Bayesian Checking of the Second Levels of Hierarchical Models",
  author    = "Bayarri, M J and Castellanos, M E",
  abstract  = "Project Euclid - mathematics and statistics online",
  journal   = "Stat. Sci.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  22,
  number    =  3,
  pages     = "322--343",
  month     =  aug,
  year      =  2007,
  keywords  = "Model checking; model criticism; objective Bayesian methods;
               p-values; conflict; empirical-Bayes; posterior predictive;
               partial posterior predictive",
  issn_alt  = "2168-8745"
}

@ARTICLE{Marshall2007-hd,
  title     = "Identifying outliers in Bayesian hierarchical models: a
               simulation-based approach",
  author    = "Marshall, E C and Spiegelhalter, D J",
  abstract  = "Project Euclid - mathematics and statistics online",
  journal   = "Bayesian Anal.",
  publisher = "International Society for Bayesian Analysis",
  volume    =  2,
  number    =  2,
  pages     = "409--444",
  month     =  jun,
  year      =  2007,
  keywords  = "Hierarchical models; Diagnostics; Outliers; Distributional
               assumptions",
  issn_alt  = "1931-6690"
}

@TECHREPORT{Gelfand1992-ow,
  title       = "Model determination using predictive distributions with
                 implementation via sampling-based methods",
  author      = "Gelfand, A E and Dey, D K and Chang, H",
  abstract    = "Abstract: Model determination is divided into the issues of
                 model adequacy and model selection. Predictive distributions
                 are used to address both issues. This seems natural since,
                 typically, prediction is a primary purpose for the chosen
                 model . A cross-validation ...",
  publisher   = "DTIC Document",
  number      =  462,
  institution = "Stanford Uni CA Dept Stat",
  year        =  1992
}

@ARTICLE{Box1980-ud,
  title     = "Sampling and {Bayes'} Inference in Scientific Modelling and
               Robustness",
  author    = "Box, George E P",
  abstract  = "Scientific learning is an iterative process employing Criticism
               and Estimation. Correspondingly the formulated model factors
               into two complementary parts--a predictive part allowing model
               criticism, and a Bayes posterior part allowing estimation.
               Implications for significance tests, the theory of precise
               measurement and for ridge estimates are considered. Predictive
               checking functions for transformation, serial correlation, bad
               values, and their relation with Bayesian options are considered.
               Robustness is seen from a Bayesian viewpoint and examples are
               given. For the bad value problem a comparison with M estimators
               is made.",
  journal   = "J. R. Stat. Soc. Ser. A",
  publisher = "Wiley for the Royal Statistical Society",
  volume    =  143,
  number    =  4,
  pages     = "383--430",
  year      =  1980
}

@ARTICLE{Bayarri1999-ty,
  title    = "Quantifying surprise in the data and model verification",
  author   = "Bayarri, M J and Berger, J O",
  journal  = "Bayes. Stat.",
  year     =  1999
}

@INPROCEEDINGS{Iwata2013-yj,
  title         = "Warped Mixtures for Nonparametric Cluster Shapes",
  booktitle     = "Conf. on Unc. in Art. Int. ({UAI})",
  author        = "Iwata, Tomoharu and Duvenaud, David and Ghahramani, Zoubin",
  abstract      = "A mixture of Gaussians fit to a single curved or
                   heavy-tailed cluster will report that the data contains many
                   clusters. To produce more appropriate clusterings, we
                   introduce a model which warps a latent mixture of Gaussians
                   to produce nonparametric cluster shapes. The possibly
                   low-dimensional latent mixture model allows us to summarize
                   the properties of the high-dimensional clusters (or density
                   manifolds) describing the data. The number of manifolds, as
                   well as the shape and dimension of each manifold is
                   automatically inferred. We derive a simple inference scheme
                   for this model which analytically integrates out both the
                   mixture parameters and the warping function. We show that
                   our model is effective for density estimation, performs
                   better than infinite Gaussian mixture models at recovering
                   the true number of clusters, and produces interpretable
                   summaries of high-dimensional datasets.",
  journal       = "arXiv [stat.ML]",
  publisher     = "arxiv.org",
  year          =  2013,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML"
}

@ARTICLE{Peel2000-pv,
  title     = "Robust mixture modelling using the t distribution",
  author    = "Peel, D and McLachlan, G J",
  abstract  = "Normal mixture models are being increasingly used to model the
               distributions of a wide variety of random phenomena and to
               cluster sets of continuous multivariate data. However, for a set
               of data containing a group or groups of observations with longer
               than normal tails or atypical observations, the use of normal
               components may unduly affect the fit of the mixture model. In
               this paper, we consider a more robust approach by modelling the
               data by a mixture of t distributions. The use of the ECM
               algorithm to fit this t mixture model is described and examples
               of its use are given in the context of clustering multivariate
               data in the presence of atypical observations in the form of
               background noise.",
  journal   = "Stat. Comput.",
  publisher = "Kluwer Academic Publishers",
  volume    =  10,
  number    =  4,
  pages     = "339--348",
  month     =  "1~" # oct,
  year      =  2000,
  issn_alt  = "1573-1375"
}

@ARTICLE{Stigler1977-dd,
  title     = "Do Robust Estimators Work with Real Data?",
  author    = "Stigler, Stephen M",
  abstract  = "Project Euclid - mathematics and statistics online",
  journal   = "Ann. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  5,
  number    =  6,
  pages     = "1055--1098",
  month     =  nov,
  year      =  1977,
  keywords  = "62-02; $M$-estimators; trimmed means; simulation; Monte Carlo;
               median; bias; adaptive estimators; skewness; kurtosis",
  issn_alt  = "2168-8966"
}

@BOOK{Rasmussen2006-ml,
  title     = "Gaussian Processes for Machine Learning",
  author    = "Rasmussen, C E and Williams, C K",
  publisher = "The MIT Press, Cambridge, MA, USA",
  year      =  2006
}

@ARTICLE{Bickel1969-ao,
  title     = "A Distribution Free Version of the Smirnov Two Sample Test in
               the p-Variate Case",
  author    = "Bickel, P J",
  abstract  = "1. Introduction. One of the classic problems of the theory of
               nonparametric inference is testing whether two samples come from
               the same or different populations. If the observations are
               univariate and we suppose only that the parent populations are
               governed by a ...",
  journal   = "Ann. Math. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  40,
  number    =  1,
  pages     = "1--23",
  month     =  "1~" # feb,
  year      =  1969
}

@INPROCEEDINGS{Hotelling1951-jd,
  title     = "A Generalized t-test and Measure of Multivariate Dispersion",
  booktitle = "Proc. 2nd Berkeley Symp. Math. Stat. and Prob.",
  author    = "Hotelling, Harold",
  abstract  = "Project Euclid - mathematics and statistics online",
  journal   = "Proceedings of the second Berkeley symposium",
  publisher = "The Regents of the University of California",
  year      =  1951,
  keywords  = "Hotelling T-squared distribution"
}

@ARTICLE{Rubin1984-tw,
  title     = "Bayesianly Justifiable and Relevant Frequency Calculations for
               the Applied Statistician",
  author    = "Rubin, Donald B",
  journal   = "Ann. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  12,
  number    =  4,
  pages     = "1151--1172",
  year      =  1984,
  keywords  = "62-07; Calibration; empirical Bayes; inference; model
               monitoring; operating characteristics; posterior predictive
               checks; stopping rules",
  issn_alt  = "2168-8966"
}

@INPROCEEDINGS{Goodman2008-ok,
  title     = "Church : a language for generative models",
  booktitle = "Conf. on Unc. in Art. Int. ({UAI})",
  author    = "Goodman, Noah D and Mansinghka, Vikash K and Roy, Daniel M and
               Bonawitz, Keith and Tenenbaum, Joshua B",
  journal   = "Proceedings of the Conference on Uncertainty in Artificial
               Intelligence (UAI)",
  year      =  2008
}

@ARTICLE{Geweke2004-yx,
  title    = "Getting It Right",
  author   = "Geweke, John",
  journal  = "J. Am. Stat. Assoc.",
  volume   =  99,
  number   =  467,
  pages    = "799--804",
  month    =  sep,
  year     =  2004
}

@ARTICLE{Gelman2013-am,
  title    = "Understanding posterior p-values",
  author   = "Gelman, Andrew",
  journal  = "Elec. J. Stat.",
  year     =  2013,
  keywords = "and phrases; bayesian inference; model checking; p-value;
              posterior; predictive check; u-value"
}

@ARTICLE{Hinton2007-eo,
  title    = "To Recognize Shapes, First Learn to Generate Images",
  author   = "Hinton, Geoffrey E",
  journal  = "Prog. Brain Res.",
  volume   =  165,
  pages    = "535--547",
  year     =  2007
}

@ARTICLE{Gelman1996-ez,
  title    = "Posterior predictive assessment of model fitness via realized
              discrepancies",
  author   = "Gelman, Andrew and Meng, Xiao-Li and Stern, Hal",
  journal  = "Stat. Sin.",
  volume   =  6,
  pages    = "733--807",
  year     =  1996,
  keywords = "and phrases; bayesian p -value; discrepancy; graphical assess-;
              ment; mixture model; model criticism; p -value; posterior
              predictive p -value; prior predictive; realized discrepancy; χ 2
              test"
}

@INPROCEEDINGS{Wilson2013-eq,
  title     = "Gaussian Process Covariance Kernels for Pattern Discovery and
               Extrapolation",
  booktitle = "Proc. Int. Conf. Machine Learn.",
  author    = "Wilson, Andrew Gordon and Adams, Ryan Prescott",
  journal   = "Proc. Int. Conf. Machine Learn.",
  year      =  2013
}

@INPROCEEDINGS{Grosse2012-zi,
  title     = "Exploiting compositionality to explore a large space of model
               structures",
  booktitle = "Conf. on Unc. in Art. Int. ({UAI})",
  author    = "Grosse, Roger B and Salakhutdinov, Ruslan and Freeman, William T
               and Tenenbaum, Joshua B",
  year      =  2012
}

@INPROCEEDINGS{Milch2005-qc,
  title     = "{BLOG}: Probabilistic models with unknown objects",
  booktitle = "Proc. Int. Joint Conf. on Artificial Intelligence",
  author    = "Milch, B and Marthi, B and Russel, S and Sontag, D and Ong, D L
               and Kolobov, A",
  journal   = "Proceedings of the International Joint Conference on Artificial
               Intelligence",
  year      =  2005
}

@ARTICLE{Robins2000-oz,
  title    = "Asymptotic Distribution of {p-values} in Composite Null Models",
  author   = "Robins, James M and van der Vaart, Aad and Venture, Valerie",
  journal  = "J. Am. Stat. Assoc.",
  volume   =  95,
  number   =  452,
  pages    = "1143--1156",
  year     =  2000,
  keywords = "asymptotic"
}

@ARTICLE{Vehtari2012-oh,
  title    = "A survey of {Bayesian} predictive methods for model assessment,
              selection and comparison",
  author   = "Vehtari, Aki and Ojanen, Janne",
  journal  = "Stat. Surv.",
  volume   =  6,
  pages    = "142--228",
  year     =  2012,
  keywords = "62-02, 62C10Bayesian, predictive, model assessment; and phrases;
              bayesian; cross-validation; decision theory; expected utility;
              information cri-; model; model assessment; predictive; selection"
}
@ARTICLE{Gretton2008-gs,
  title    = "A Kernel Method for the {two-sample} Problem",
  author   = "Gretton, Arthur and Borgwardt, Karsten M and Rasch, Malte J and
              Sch{\"{o}}lkopf, Berhard and Smola, Alexander",
  journal  = "Journal of Machine Learning Research",
  volume   =  1,
  pages    = "1--10",
  year     =  2008
}
@ARTICLE{Gretton2012-ss,
  title   = "A Kernel Two Sample Test",
  author  = "Gretton, Arthur and Borgwardt, Karsten M and Rasch, Malte J and
             Sch{\"{o}}lkopf, Bernhard and Smola, Alexander",
  journal = "Journal of Machine Learning Research",
  volume  =  13,
  pages   = "723--773",
  year    =  2012
}
@ARTICLE{Hinton2006-yw,
  title    = "A fast learning algorithm for deep belief nets",
  author   = "Hinton, Geoffrey E and Osindero, Simon and Teh, Yee Whye",
  journal  = "Neural Comput.",
  volume   =  18,
  number   =  7,
  pages    = "1527--1554",
  year     =  2006
}

@INPROCEEDINGS{Thornton2013-zg,
  title     = "{Auto-WEKA}: Combined Selection and Hyperparameter Optimization
               of Classification Algorithms",
  booktitle = "Proc. Int. Conf. on Knowledge Discovery and Data Mining",
  author    = "Thornton, Chris and Hutter, Frank and Hoos, Holger H and
               Leyton-Brown, Kevin",
  abstract  = "Abstract Many different machine learning algorithms exist;
               taking into account each algorithm's hyperparameters, there is a
               staggeringly large number of possible alternatives overall. We
               consider the problem of simultaneously selecting a learning
               algorithm and ...",
  journal   = "Proceedings of the 19th",
  publisher = "ACM",
  pages     = "847--855",
  series    = "KDD '13",
  year      =  2013,
  address   = "New York, NY, USA",
  keywords  = "hyperparameter optimization, model selection, weka"
}

@INPROCEEDINGS{Lloyd2014-nz,
  title         = "Automatic Construction and {Natural-Language} Description of
                   Nonparametric Regression Models",
  booktitle     = "Association for the Advancement of Artificial Intelligence
                   ({AAAI})",
  author        = "Lloyd, James Robert and Duvenaud, David and Grosse, Roger
                   and Tenenbaum, Joshua B and Ghahramani, Zoubin",
  abstract      = "This paper presents the beginnings of an automatic
                   statistician, focusing on regression problems. Our system
                   explores an open-ended space of possible statistical models
                   to discover a good explanation of the data, and then
                   produces a detailed report with figures and natural-language
                   text. Our approach treats unknown functions
                   nonparametrically using Gaussian processes, which has two
                   important consequences. First, Gaussian processes model
                   functions in terms of high-level properties (e.g.
                   smoothness, trends, periodicity, changepoints). Taken
                   together with the compositional structure of our language of
                   models, this allows us to automatically describe functions
                   through a decomposition into additive parts. Second, the use
                   of flexible nonparametric models and a rich language for
                   composing them in an open-ended manner also results in
                   state-of-the-art extrapolation performance evaluated over 13
                   real time series data sets from various domains.",
  journal       = "arXiv [stat.ML]",
  month         =  jul,
  year          =  2014,
  conference    = "Association for the Advancement of Artificial Intelligence
                   (AAAI)",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML"
}
@ARTICLE{OHagan2003-bc,
  title     = "{HSSS} Model Criticism",
  author    = "O'Hagan, A",
  abstract  = "There has been an enormous surge in practical applications of
               Bayesian statistics in the last decade. The primary reason for
               this has been the development of MCMC tools for computing
               posterior inferences. HSSS has played a major part in that
               process, through helping to ...",
  journal   = "Highly Structured Stochastic Systems",
  publisher = "Oxford University Press",
  pages     = "423--444",
  year      =  2003
}
@BOOK{Gelman2013-st,
  title     = "Bayesian Data Analysis, Third Edition",
  author    = "Gelman, A and Carlin, J B and Stern, H S and Dunson, D B and
               Vehtari, A and Rubin, D B",
  abstract  = "Now in its third edition, this classic book is widely considered
               the leading text on Bayesian methods, lauded for its accessible,
               practical approach to analyzing data and solving research
               problems. Bayesian Data Analysis , Third Edition continues to
               take an applied ...",
  publisher = "Taylor \& Francis",
  series    = "Chapman \& Hall/CRC Texts in Statistical Science",
  year      =  2013
}
@ARTICLE{Cook1982-ia,
  title     = "Residuals and influence in regression",
  author    = "Cook, R D and Weisberg, S",
  journal   = "Mon. on Stat. and App. Prob.,",
  publisher = "conservancy.umn.edu",
  year      =  1982
}
@ARTICLE{Borgwardt2006-gy,
  title       = "Integrating structured biological data by Kernel Maximum Mean
                 Discrepancy",
  author      = "Borgwardt, Karsten M and Gretton, Arthur and Rasch, Malte J
                 and Kriegel, Hans-Peter and Sch{\"{o}}lkopf, Bernhard and
                 Smola, Alex J",
  affiliation = "Institute for Computer Science, Ludwig-Maximilians-University,
                 Munich, Germany. kb@dbs.ifi.lmu.de",
  abstract    = "MOTIVATION: Many problems in data integration in
                 bioinformatics can be posed as one common question: Are two
                 sets of observations generated by the same distribution? We
                 propose a kernel-based statistical test for this problem,
                 based on the fact that two distributions are different if and
                 only if there exists at least one function having different
                 expectation on the two distributions. Consequently we use the
                 maximum discrepancy between function means as the basis of a
                 test statistic. The Maximum Mean Discrepancy (MMD) can take
                 advantage of the kernel trick, which allows us to apply it not
                 only to vectors, but strings, sequences, graphs, and other
                 common structured data types arising in molecular biology.
                 RESULTS: We study the practical feasibility of an MMD-based
                 test on three central data integration tasks: Testing
                 cross-platform comparability of microarray data, cancer
                 diagnosis, and data-content based schema matching for two
                 different protein function classification schemas. In all of
                 these experiments, including high-dimensional ones, MMD is
                 very accurate in finding samples that were generated from the
                 same distribution, and outperforms its best competitors.
                 Conclusions: We have defined a novel statistical test of
                 whether two samples are from the same distribution, compatible
                 with both multivariate and structured data, that is fast, easy
                 to implement, and works well, as confirmed by our experiments.
                 AVAILABILITY: http://www.dbs.ifi.lmu.de/~borgward/MMD.",
  journal     = "Bioinformatics",
  volume      =  22,
  number      =  14,
  pages       = "e49--57",
  month       =  "15~" # jul,
  year        =  2006,
  issn_alt    = "1367-4803"
}
@INPROCEEDINGS{Meulders1998-xo,
  title     = "Generalizing the probability matrix decomposition model: an
               example of Bayesian model checking and model expansion",
  booktitle = "Assumptions, Robustness, and Estimation Methods in Multivariate
               Modeling",
  author    = "Meulders, Michel and Gelman, Andrew and Van Mechelen, Iven and
               De Boeck, Paul",
  abstract  = "CiteSeerX - Document Details (Isaac Councill, Lee Giles, Pradeep
               Teregowda): Probability matrix decomposition (PMD) models can be
               used to explain observed associations between two sets of
               elements. More specifically, observed associations are modeled
               as a deterministic function of B latent Bernoulli variables that
               are realized for each element. To estimate the parameters of
               this model, a sample of the posterior distribution is computed
               with a data augmentation algorithm. The obtained posterior
               sample can also be used to assess the fit of the model with the
               technique of posterior predictive checks. In this paper a PMD
               model is applied to data on psychiatric diagnosis. In checking
               the model for this analysis, we focus on the appropriateness of
               the prior distribution for a set of latent parameters. Based on
               the posterior distribution for the values of the parameters
               corresponding to the observed data, we conclude that a
               relatively flat prior distribution is inappropriate. In order to
               solve this problem, a mixture prior density with two beta
               distributed components ...",
  publisher = "Citeseer",
  year      =  1998
}
@BOOK{McLachlan2004-qz,
  title     = "Finite Mixture Models",
  author    = "McLachlan, G and Peel, D",
  abstract  = "An up-to-date, comprehensive account of major issues in finite
               mixture modeling This volume provides an up-to-date account of
               the theory and applications of modeling via finite mixture
               distributions. With an emphasis on the applications of mixture
               models in both ...",
  publisher = "Wiley",
  series    = "Wiley series in probability and statistics: Applied probability
               and statistics",
  year      =  2004
}
@ARTICLE{Snelson2007-mj,
  title   = "Local and global sparse gaussian process approximations",
  author  = "Snelson, Edward and Ghahramani, Zoubin",
  journal = "Artificial Intelligence and Statistics",
  year    =  2007
}
@ARTICLE{Vanhatalo2012-yr,
  title         = "Modelling local and global phenomena with sparse Gaussian
                   processes",
  author        = "Vanhatalo, Jarno and Vehtari, Aki",
  abstract      = "Much recent work has concerned sparse approximations to
                   speed up the Gaussian process regression from the
                   unfavorable O(n3) scaling in computational time to O(nm2).
                   Thus far, work has concentrated on models with one
                   covariance function. However, in many practical situations
                   additive models with multiple covariance functions may
                   perform better, since the data may contain both long and
                   short length-scale phenomena. The long length-scales can be
                   captured with global sparse approximations, such as fully
                   independent conditional (FIC), and the short length-scales
                   can be modeled naturally by covariance functions with
                   compact support (CS). CS covariance functions lead to
                   naturally sparse covariance matrices, which are
                   computationally cheaper to handle than full covariance
                   matrices. In this paper, we propose a new sparse Gaussian
                   process model with two additive components: FIC for the long
                   length-scales and CS covariance function for the short
                   length-scales. We give theoretical and experimental results
                   and show that under certain conditions the proposed model
                   has the same computational complexity as FIC. We also
                   compare the model performance of the proposed model to
                   additive models approximated by fully and partially
                   independent conditional (PIC). We use real data sets and
                   show that our model outperforms FIC and PIC approximations
                   for data sets with two additive phenomena.",
  journal       = "arXiv preprint 1206.3290",
  publisher     = "arxiv.org",
  month         =  "13~" # jun,
  year          =  2012,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}
@ARTICLE{Blei2014-uz,
  title    = "Build, Compute, Critique, Repeat: Data Analysis with Latent
              Variable Models",
  author   = "Blei, David M",
  abstract = "We survey latent variable models for solving data-analysis
              problems. A latent variable model is a probabilistic model that
              encodes hidden patterns in the data. We uncover these patterns
              from their conditional distribution and use them to summarize
              data and form predictions. Latent variable models are important
              in many fields, including computational biology, natural language
              processing, and social network analysis. Our perspective is that
              models are developed iteratively: We build a model, use it to
              analyze data, assess how it succeeds and fails, revise it, and
              repeat. We describe how new research has transformed these
              essential activities. First, we describe probabilistic graphical
              models, a language for formulating latent variable models.
              Second, we describe mean field variational inference, a generic
              algorithm for approximating conditional distributions. Third, we
              describe how to use our analyses to solve problems: exploring the
              data, forming predictions, and pointing us in the direction of
              improved models.",
  journal  = "Annual Review of Statistics and Its Application",
  volume   =  1,
  number   =  1,
  pages    = "203--232",
  year     =  2014
}
@ARTICLE{Friedman1979-ur,
  title     = "Multivariate Generalizations of the {Wald-Wolfowitz} and Smirnov
               {Two-Sample} Tests",
  author    = "Friedman, Jerome H. and Rafsky, Lawrence C.",
  abstract  = "Multivariate generalizations of the Wald-Wolfowitz runs
               statistic and the Smirnov maximum deviation statistic for the
               two-sample problem are presented. They are based on the minimal
               spanning tree of the pooled sample points. Some null
               distribution results are derived and a simulation study of power
               is reported.",
  journal   = "Ann. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  7,
  number    =  4,
  pages     = "697--717",
  month     =  "1~" # jul,
  year      =  1979
}
@ARTICLE{Muller1997-vs,
  title     = "Integral Probability Metrics and Their Generating Classes of
               Functions",
  author    = "M{\"u}ller, Alfred",
  abstract  = "We consider probability metrics of the following type: for a
               class F of functions and probability measures P, Q we define
               $d_\textbackslash{}germF(P,\textbackslash{}
               Q)\textbackslash{}coloneq
               \textbackslash{}textsup_f\textbackslash{}in
               \textbackslash{}germF|\textbackslash{}int
               f\textbackslash{},dP-\textbackslash{}int f\textbackslash{},dQ|$
               . A unified study of such integral probability metrics is given.
               We characterize the maximal class of functions that generates
               such a metric. Further, we show how some interesting properties
               of these probability metrics arise directly from conditions on
               the generating class of functions. The results are illustrated
               by several examples, including the Kolmogorov metric, the Dudley
               metric and the stop-loss metric.",
  journal   = "Adv. Appl. Probab.",
  publisher = "Applied Probability Trust",
  volume    =  29,
  number    =  2,
  pages     = "429--443",
  month     =  "1~" # jun,
  year      =  1997
}
@ARTICLE{Gelman2013-sz,
  title       = "Philosophy and the practice of Bayesian statistics",
  author      = "Gelman, Andrew and Shalizi, Cosma Rohilla",
  affiliation = "Department of Statistics and Department of Political Science,
                 Columbia University, New York, New York 10027, USA.
                 gelman@stat.columbia.edu",
  abstract    = "A substantial school in the philosophy of science identifies
                 Bayesian inference with inductive inference and even
                 rationality as such, and seems to be strengthened by the rise
                 and practical success of Bayesian statistics. We argue that
                 the most successful forms of Bayesian statistics do not
                 actually support that particular philosophy but rather accord
                 much better with sophisticated forms of
                 hypothetico-deductivism. We examine the actual role played by
                 prior distributions in Bayesian models, and the crucial
                 aspects of model checking and model revision, which fall
                 outside the scope of Bayesian confirmation theory. We draw on
                 the literature on the consistency of Bayesian updating and
                 also on our experience of applied work in social science.
                 Clarity about these matters should benefit not just philosophy
                 of science, but also statistical practice. At best, the
                 inductivist view has encouraged researchers to fit and compare
                 models without checking them; at worst, theorists have
                 actively discouraged practitioners from performing model
                 checking because it does not fit into their framework.",
  journal     = "Br. J. Math. Stat. Psychol.",
  volume      =  66,
  number      =  1,
  pages       = "8--38",
  month       =  feb,
  year        =  2013,
  issn_alt    = "0007-1102"
}
@ARTICLE{Jordan2013-uv,
  title     = "On statistics, computation and scalability",
  author    = "Jordan, Michael I",
  abstract  = "Project Euclid - mathematics and statistics online",
  journal   = "Bernoulli",
  publisher = "Bernoulli Society for Mathematical Statistics and Probability",
  volume    =  19,
  number    =  4,
  pages     = "1378--1390",
  month     =  sep,
  year      =  2013
}
@BOOK{Stein1999-ps,
  title     = "Interpolation of Spatial Data: Some Theory for Kriging",
  author    = "Stein, M L",
  publisher = "Springer New York",
  series    = "Springer Series in Statistics",
  year      =  1999
}
@BOOK{Kallenberg2005-ec,
  title     = "Probabilistic symmetries and invariance principles",
  author    = "Kallenberg, Olav",
  publisher = "Springer",
  year      =  2005
}
@ARTICLE{Zhe2013-tv,
  title   = "{DinTucker} : Scaling up Gaussian process models on
             multidimensional arrays with billions of elements",
  author  = "Zhe, Shandian and Park, Youngja and Molloy, Ian",
  journal = "arXiv preprint 1311.2663",
  year    =  2013
}
@INPROCEEDINGS{Adams2010-ln,
  title     = "Incorporating Side Information in Probabilistic Matrix
               Factorization with Gaussian Processes",
  booktitle = "Proceedings of the {Twenty-Sixth} Annual Conference on
               Uncertainty in Artificial Intelligence ({UAI})",
  author    = "Adams, Ryan Prescott and Dahl, George E and Murray, Iain",
  pages     = "1--9",
  year      =  2010
}
@inproceedings{durante2014bayesian,
  title="Bayesian Logistic Gaussian Process Models for Dynamic Networks",
  author="Durante, Daniele and Dunson, David",
  booktitle="Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics",
  pages="194--201",
  year=2014
}
@ARTICLE{Vehtari2012-oh,
  title    = "A survey of Bayesian predictive methods for model assessment,
              selection and comparison",
  author   = "Vehtari, Aki and Ojanen, Janne",
  journal  = "Stat. Surv.",
  volume   =  6,
  pages    = "142--228",
  year     =  2012,
  keywords = "62-02, 62C10Bayesian, predictive, model assessment; and phrases;
              bayesian; cross-validation; decision theory; expected utility;
              information cri-; model; model assessment; predictive; selection"
}
@ARTICLE{Lean1995-vp,
  title     = "Reconstruction of solar irradiance since 1610: Implications for
               climate change",
  author    = "Lean, J and Beer, J and Bradley, R",
  journal   = "Geophys. Res. Lett.",
  publisher = "AGU AMERICAN GEOPHYSICAL UNION",
  volume    =  22,
  number    =  23,
  pages     = "3195--3198",
  year      =  1995
}
@ARTICLE{Breiman2001-at,
  title     = "Random Forests",
  author    = "Breiman, Leo",
  abstract  = "Random forests are a combination of tree predictors such that
               each tree depends on the values of a random vector sampled
               independently and with the same distribution for all trees in
               the forest. The generalization error for forests converges a.s.
               to a limit as the number of trees in the forest becomes large.
               The generalization error of a forest of tree classifiers depends
               on the strength of the individual trees in the forest and the
               correlation between them. Using a random selection of features
               to split each node yields error rates that compare favorably to
               Adaboost (Y. Freund \& R. Schapire, Machine Learning:
               Proceedings of the Thirteenth International conference, ***,
               148-156), but are more robust with respect to noise. Internal
               estimates monitor error, strength, and correlation and these are
               used to show the response to increasing the number of features
               used in the splitting. Internal estimates are also used to
               measure variable importance. These ideas are also applicable to
               regression.",
  journal   = "Mach. Learn.",
  publisher = "Kluwer Academic Publishers",
  volume    =  45,
  number    =  1,
  pages     = "5--32",
  month     =  "1~" # oct,
  year      =  2001,
  issn_alt  = "1573-0565"
}
@INPROCEEDINGS{Murray2010-hv,
  title     = "Slice sampling covariance hyperparameters of latent Gaussian
               models",
  booktitle = "Advances in Neural Information Processing Systems ({NIPS})",
  author    = "Murray, Iain and Adams, Ryan Prescott",
  year      =  2010
}
@ARTICLE{Skilling2006-ez,
  title     = "Nested sampling for general Bayesian computation",
  author    = "Skilling, John",
  abstract  = "Project Euclid - mathematics and statistics online",
  journal   = "Bayesian Anal.",
  publisher = "International Society for Bayesian Analysis",
  volume    =  1,
  number    =  4,
  pages     = "833--859",
  month     =  dec,
  year      =  2006,
  keywords  = "Bayesian computation; evidence; marginal likelihood; algorithm;
               nest; annealing; phase change; model selection",
  issn_alt  = "1931-6690"
}
@UNPUBLISHED{Feroz2013-wk,
  title         = "Exploring {Multi-Modal} Distributions with Nested Sampling",
  author        = "Feroz, F and Skilling, J",
  abstract      = "In performing a Bayesian analysis, two difficult problems
                   often emerge. First, in estimating the parameters of some
                   model for the data, the resulting posterior distribution may
                   be multi-modal or exhibit pronounced (curving) degeneracies.
                   Secondly, in selecting between a set of competing models,
                   calculation of the Bayesian evidence for each model is
                   computationally expensive using existing methods such as
                   thermodynamic integration. Nested Sampling is a Monte Carlo
                   method targeted at the efficient calculation of the
                   evidence, but also produces posterior inferences as a
                   by-product and therefore provides means to carry out
                   parameter estimation as well as model selection. The main
                   challenge in implementing Nested Sampling is to sample from
                   a constrained probability distribution. One possible
                   solution to this problem is provided by the Galilean Monte
                   Carlo (GMC) algorithm. We show results of applying Nested
                   Sampling with GMC to some problems which have proven very
                   difficult for standard Markov Chain Monte Carlo (MCMC) and
                   down-hill methods, due to the presence of large number of
                   local minima and/or pronounced (curving) degeneracies
                   between the parameters. We also discuss the use of Nested
                   Sampling with GMC in Bayesian object detection problems,
                   which are inherently multi-modal and require the evaluation
                   of Bayesian evidence for distinguishing between true and
                   spurious detections.",
  month         =  "19~" # dec,
  year          =  2013,
  archivePrefix = "arXiv",
  primaryClass  = "astro-ph.IM",
  eprint        = "1312.5638"
}
@ARTICLE{Feroz2013-dj,
  title         = "Importance Nested Sampling and the {MultiNest} Algorithm",
  author        = "Feroz, F and Hobson, M P and Cameron, E and Pettitt, A N",
  abstract      = "Bayesian inference involves two main computational
                   challenges. First, in estimating the parameters of some
                   model for the data, the posterior distribution may well be
                   highly multi-modal: a regime in which the convergence to
                   stationarity of traditional Markov Chain Monte Carlo (MCMC)
                   techniques becomes incredibly slow. Second, in selecting
                   between a set of competing models the necessary estimation
                   of the Bayesian evidence for each is, by definition, a
                   (possibly high-dimensional) integration over the entire
                   parameter space; again this can be a daunting computational
                   task, although new Monte Carlo (MC) integration algorithms
                   offer solutions of ever increasing efficiency. Nested
                   sampling (NS) is one such contemporary MC strategy targeted
                   at calculation of the Bayesian evidence, but which also
                   enables posterior inference as a by-product, thereby
                   allowing simultaneous parameter estimation and model
                   selection. The widely-used MultiNest algorithm presents a
                   particularly efficient implementation of the NS technique
                   for multi-modal posteriors. In this paper we discuss
                   importance nested sampling (INS), an alternative summation
                   of the MultiNest draws, which can calculate the Bayesian
                   evidence at up to an order of magnitude higher accuracy than
                   `vanilla' NS with no change in the way MultiNest explores
                   the parameter space. This is accomplished by treating as a
                   (pseudo-)importance sample the totality of points collected
                   by MultiNest, including those previously discarded under the
                   constrained likelihood sampling of the NS algorithm. We
                   apply this technique to several challenging test problems
                   and compare the accuracy of Bayesian evidences obtained with
                   INS against those from vanilla NS.",
  journal       = "arXiv preprint 1306.2144",
  month         =  "10~" # jun,
  year          =  2013,
  archivePrefix = "arXiv",
  primaryClass  = "astro-ph.IM"
}
@ARTICLE{Gerrish_undated-zh,
  title   = "Black Box Variational Inference",
  author  = "Gerrish, Sean and Blei, David M",
  journal = "arXiv preprint arXiv:1401.0118"
}
@ARTICLE{Hensman2013-ox,
  title         = "Gaussian Processes for Big Data",
  author        = "Hensman, James and Fusi, Nicolo and Lawrence, Neil D",
  abstract      = "We introduce stochastic variational inference for Gaussian
                   process models. This enables the application of Gaussian
                   process (GP) models to data sets containing millions of data
                   points. We show how GPs can be vari- ationally decomposed to
                   depend on a set of globally relevant inducing variables
                   which factorize the model in the necessary manner to perform
                   variational inference. Our ap- proach is readily extended to
                   models with non-Gaussian likelihoods and latent variable
                   models based around Gaussian processes. We demonstrate the
                   approach on a simple toy problem and two real world data
                   sets.",
  journal       = "arXiv preprint 1309.6835",
  month         =  "26~" # sep,
  year          =  2013,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}
@ARTICLE{Bishop2006-yy,
  title     = "Pattern recognition and machine learning",
  author    = "Bishop, C M",
  abstract  = "Machine learning is a key technology in bioinformatics,
               especially in the analysis of`` big data'' in bioinformatics.
               This course gives an overview of basic concepts, algorithms, and
               applications in machine learning , including topics such as
               classification, linear regression, ...",
  publisher = "soic.iupui.edu",
  year      =  2006
}
@ARTICLE{Swersky2014-aw,
  title         = "{Freeze-Thaw} Bayesian Optimization",
  author        = "Swersky, Kevin and Snoek, Jasper and Adams, Ryan Prescott",
  abstract      = "In this paper we develop a dynamic form of Bayesian
                   optimization for machine learning models with the goal of
                   rapidly finding good hyperparameter settings. Our method
                   uses the partial information gained during the training of a
                   machine learning model in order to decide whether to pause
                   training and start a new model, or resume the training of a
                   previously-considered model. We specifically tailor our
                   method to machine learning problems by developing a novel
                   positive-definite covariance kernel to capture a variety of
                   training curves. Furthermore, we develop a Gaussian process
                   prior that scales gracefully with additional temporal
                   observations. Finally, we provide an information-theoretic
                   framework to automate the decision process. Experiments on
                   several common machine learning models show that our
                   approach is extremely effective in practice.",
  journal       = "arXiv preprint 1406.3896",
  month         =  "16~" # jun,
  year          =  2014,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML"
}
@ARTICLE{Ghahramani2002-by,
  title     = "Bayesian monte carlo",
  author    = "Ghahramani, Z and Rasmussen, C E",
  abstract  = "Abstract We investigate Bayesian alternatives to classical Monte
               Carlo methods for evaluating integrals. Bayesian Monte Carlo
               (BMC) allows the incorporation of prior knowledge, such as
               smoothness of the integrand, into the estimation. In a simple
               problem ...",
  journal   = "Adv. Neural Inf. Process. Syst.",
  publisher = "machinelearning.wustl.edu",
  year      =  2002
}
@ARTICLE{OHagan1991-wg,
  title   = "{Bayes-Hermite} quadrature",
  author  = "O'Hagan, A",
  journal = "J. Stat. Plan. Inference",
  volume  =  29,
  pages   = "245--260",
  year    =  1991
}
@INPROCEEDINGS{Hennig2012-wv,
  title     = "{Quasi-Newton} Methods: A New Direction",
  booktitle = "Proceedings of the International Conference on Machine Learning
               ({ICML})",
  author    = "Hennig, Philipp and Kiefel, Martin",
  year      =  2012
}
@UNPUBLISHED{Karpathy_undated-ww,
  title  = "Deep {Visual-Semantic} Alignments for Generating Image Descriptions",
  author = "Karpathy, Andrej and Fei-Fei, Li",
  year = 2014
}
}@article{green2011prediction,
  title={Prediction API: Every app a smart app},
  author={Green, T and others},
  journal={Google Developers Blog, Apr},
  volume={21},
  year={2011}
}
@ARTICLE{Van_der_Vaart2011-wo,
  title     = "Information Rates of Nonparametric Gaussian Process Methods",
  author    = "van der Vaart, Aad and van Zanten, Harry",
  journal   = "J. Mach. Learn. Res.",
  publisher = "JMLR.org",
  volume    =  12,
  pages     = "2095--2119",
  month     =  jul,
  year      =  2011
}
@INPROCEEDINGS{Chwialkowski2014-pe,
  title         = "A Wild Bootstrap for Degenerate Kernel Tests",
  booktitle     = "Advances in Neural Information Processing Systems ({NIPS})",
  author        = "Chwialkowski, Kacper and Sejdinovic, Dino and Gretton,
                   Arthur",
  abstract      = "A wild bootstrap method for nonparametric hypothesis tests
                   based on kernel distribution embeddings is proposed. This
                   bootstrap method is used to construct provably consistent
                   tests that apply to random processes, for which the naive
                   permutation-based bootstrap fails. It applies to a large
                   group of kernel tests based on V-statistics, which are
                   degenerate under the null hypothesis, and non-degenerate
                   elsewhere. To illustrate this approach, we construct a
                   two-sample test, an instantaneous independence test and a
                   multiple lag independence test for time series. In
                   experiments, the wild bootstrap gives strong performance on
                   synthetic examples, on audio data, and in performance
                   benchmarking for the Gibbs sampler.",
  year          =  2014,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML"
}
@BOOK{Lovasz2012-df,
  title     = "Large Networks and Graph Limits",
  author    = "Lov\'{a}sz, L",
  abstract  = "Recently, it became apparent that a large number of the most
               interesting structures and phenomena of the world can be
               described by networks . To develop a mathematical theory of very
               large networks is an important challenge. This book describes
               one recent approach ...",
  publisher = "American Mathematical Society",
  series    = "American Mathematical Society colloquium publications",
  year      =  2012
}
@ARTICLE{Caron2014-on,
  title   = "Bayesian nonparametric models of sparse and exchangeable random
             graphs",
  author  = "Caron, Francois and Fox, Emily B",
  journal = "arXiv preprint 1401.1137",
  year    =  2014
}
