\input{common/header.tex}
\inbpdocument

\chapter{Introduction}
\label{ch:intro}

This thesis concerns the challenge of drawing inferences from uncertain data.
We take a probabilistic and Bayesian approach throughout  which is a natural extension of drawing logical inferences from certain information~\citep[e.g.][]{Jaynes2003-jh}.
To perform probabilistic inference faithfully one must be able to express all of one's prior information about some situation in the form of a probability distribution.
In chapters~\ref{ch:networks}~and~\ref{ch:arrays} we demonstrate how to construct probability distribution that respect symmetry conditions appropriate for data in the form of networks or databases.
In chapters~\ref{ch:construction}~and~\ref{ch:description} we develop expressive probability distributions over functions and demonstrate that they express patterns that can be easily understood by humans.
In chapter~\ref{ch:gefcom} we demonstrate the application of these distributions over functions to a data mining competition.
Finally, chapter~\ref{ch:criticism} considers the problem of criticising a probabilistic model when we have reason to believe that we were either unable to faithfully express our prior information probabilistically or perform inference to a sufficient level of accuracy.
We elaborate on these points in the rest of this chapter.

\section{Bayesians and frequentists}

All chapters of this thesis apart from chapters~\ref{ch:gefcom}~and~\ref{ch:criticism} take an entirely (or at least approximately) Bayesian (or more generally probabilistic) approach to statistical inference.
The situations in which it is wise to perform inference in a Bayesian manner as opposed to using a method justified by its frequentist properties are commonly poorly understood.
As a result, many people still talk about a Bayesians versus frequentists debate as if one paradigm could be declared better than the other\footnotemark{}.
I will therefore devote a small amount of space to the justification of Bayesian inference and also discuss its limitations.
\footnotetext{See for example a recent blog post on the subject \url{http://lesswrong.com/lw/jne/a_fervent_defense_of_frequentist_statistics/} which caused some ill informed debate around the internet \eg \url{http://www.reddit.com/r/statistics/comments/1ye692/a_fervent_defense_of_frequentist_statistics/} and \url{https://news.ycombinator.com/item?id=7263490}}

How should we perform rational inferences with uncertain information?
A solution to this question is provided by Cox's theorem \citep{Cox1946-qc} which derives a calculus of reasoning about the plausability of statements from three simple axioms of rational behaviour.
A version of these axioms are colloquially stated by \citet{Jaynes2003-jh} as
\begin{enumerate}
  \item Representation of degrees of plausability by real numbers
  \item Qualitative correspondence with common sense
  \item Consistency [of reasoning]
\end{enumerate}
The calculus that one can logically derive from these axioms is isomorphic to probability theory; plausabilities satisfy the sum and product rules that are typically stated as axioms of probability theory.

This result tells us that if we can accurately specify our beliefs as probability distributions then there is only one way to rationally (in the sense of Cox's axioms) update those beliefs in the light of new data \ie by using probability theory (which includes Bayes' rule).
Thus, we are compelled to perform inference using the rules of probability theory when the following three conditions hold
\begin{enumerate}
  \item We are willing to accept Cox's axioms
  \item We can faithfully express our prior beliefs about a problem using probability distributions
  \item Exact probabilistic inference or a sufficiently close approximation is computationally tractable
\end{enumerate}
There are some who have argued against Cox's axioms (see comments in \citet{Jaynes2003-jh}) but no widely accepted alternatives currently exist.
Conditions 2 and 3 however are often false; in response one may wish to develop or use a method with frequentist guarantees or one could attempt research which makes them true in a larger number of situations\footnotemark{}.
This thesis is concerned with the latter strategy.

\footnotetext{
Or one could attempt to revise statistical theory in such a way as to take account of computational and epistemic limitations.
These lines of enquiry are potentially very fruitful but still in their infancy \citep[e.g.][]{Jordan2013-uv, Vehtari2012-oh}.
}

%Chapters~\ref{ch:networks}~and~\ref{ch:arrays} are concerned with the problem of specifying an appropriate mathematical form of probabilistic model based on symmetry arguments.
%Chapters~\ref{ch:construction}~and~\ref{ch:description} are concerned with producing probability distributions over functions that better match our prior beliefs about the types of function we expect to encounter in data.
%Chapters~\ref{ch:gefcom}~and~\ref{ch:criticism} consider problems where we have good reason to believe that at leats one of conditions 2 and 3 do not hold.
%In particular, chapter~\ref{ch:gefcom} applies the techniques of chapter~\ref{ch:construction} to a data mining competition which reveals some areas where improvements can be made.
%Chapter~\ref{ch:criticism} develops new techniques to investigate probabilistic models where we know that we were unable to specify our prior beliefs faithfully.

\section{An introduction to exchangeability and representation theorems}

Suppose one receives a sequence of data $(X_1, X_2, \dots)$ and we wish to specify our beliefs about this data with a probability distribution.
An often reasonable prior assumption is that we do not believe the order of the data contains any information, or rather that any probability distribution encoding our beliefs about this data should be invariant to reorderings of the data.
That is, we may often believe that our data is an exchangeable sequence:
\[
  \label{eq:intro:exch}
  (X_1, X_2, \dots) \eqdist (X_{p(1)}, X_{p(2)}, \dots) \qquad \forall\, p\in\SGinf
\]
where $\eqdist$ denotes equality in distribution, and $\SGinf$ is the set of all permutations of $\Nats$ which permute at most a finite number of elements.

de Finetti's theorem \citep[e.g.][]{Kallenberg2005-ec} characterises all probability distributions of sequences with this property;  $(X_i)_{i\in\Nats}$ is an exchangeable sequence if and only if there exists a random probability measure $\AHfunction$ such that $\darray_1,\darray_2,\ldots\simiid\AHfunction$.
In other words, observations are conditionally \iid given some random $\AHfunction$.
For the purposes of probabilistic modelling, this implies that our beliefs about exchangeable sequences of data can be expressed in the form of a distribution over probability measures.
%A note on terminology: we will refer to de Finetti's theorem and related results as representation theorems. 

In chapter~\ref{ch:networks} we demonstrate that a generalisation of de Finetti's theorem due to Aldous and Hoover can be used to construct probability distributions of networks and arrays where weaker symmetry assumptions are appropriate.
In chapter~\ref{ch:arrays} we use results by Kallenberg to extend the results of Aldous and Hoover to the case of probability dsitributions for general databases or arbitrary relational data.
We also demonstrate how these representation theorems are compatible with other formalisms and assumptions \eg longitudinal / time-varying data or conditional probability distributions.

\section{An introduction to using Gaussian processes to model functions}

There are a great many (and many great) introductions to Gaussian processes; the most comprehensive at the moment is \citet{Rasmussen2006-ml}.
Rather than repeat them, I will give an intuitive derivation of Gaussian processes starting from linear regression.
Those with an understanding of Gaussian processes may safely skip this section.% and those interested in further details are directed to the following reference books\fTBD{cite Rasmussed and Williams and maybe Kallenberg as a nice (lol) contrast}.

Let us consider a simple version of Bayesian linear regression.
Suppose we have a data set $D$ consisting of real-valued (input, output) pairs denoted by $D = \{(x_i, y_i) \in \Reals^2 : i = 1,\dots,n\}$.
A simple probabilistic linear regression model of this data could be of the form.
\[
  m &\sim \Normal(0,1) \label{eq:intro:prior_m} \\
  \varepsilon_i &\simiid \Normal(0,\sigma_\varepsilon^2) \\
  y_i &= m x_i + \varepsilon_i
\]
\ie we believe our data will be linearly related by a line passing through the origin.

For this prior belief it is trivial to update our beliefs in the light of data according to the calculus of probability theory upon which we find
\[
  m \given D &\sim \Normal\bigg(\frac{\sum_i x_i y_i}{\sum_i x_i^2 + \sigma_\varepsilon^2}, \frac{\sigma_\varepsilon^2}{\sum_i x_i^2 + \sigma_\varepsilon^2}\bigg) \label{eq:intro:posterior_m} \\
  y_j \given D &\sim \Normal\bigg(\frac{x_j \sum_i x_i y_i}{\sum_i x_i^2 + \sigma_\varepsilon^2}, \frac{x_j^2 \sigma_\varepsilon^2}{\sum_i x_i^2 + \sigma_\varepsilon^2} + \sigma_\varepsilon^2\bigg) \quad \textrm{where } j \notin \{1,\dots,n\}.
\]
The updates of the prior belief for $m$ \eqref{eq:intro:prior_m} into the posterior belief \eqref{eq:intro:posterior_m} is demonstrated visually in figure~\ref{fig:intro:lin_reg}; as more data is observed one's belief about the parameter $m$ becomes more concentrated.

\begin{figure}[ht]
\centering
\includegraphics[width=0.3\columnwidth]{\introfigsdir/prior}
\includegraphics[width=0.3\columnwidth]{\introfigsdir/bayes_5}
\includegraphics[width=0.3\columnwidth]{\introfigsdir/bayes_15}
\caption[Illustration of Bayesian linear regression.]{
Bayesian linear regression.
\textit{Left}: Visualisation of prior distribution on gradient $m$.
Lines are random samples from the prior probability distribution, the red shading is a representation of the probability distribution.
\textit{Middle}: Visualiation of the posterior distribution for the gradient $m$ after 5 samples of data have been observed.
\textit{Right}: The posterior after 15 observations.
}
\label{fig:intro:lin_reg}
\end{figure}

We now rewrite the equations for the noisy linear relationship between $y$ and $x$ in an equivalent form
\[
  y_i &\sim \mathcal{N}(0, x_i^2 + \sigma_\varepsilon^2) \\
  \Cov(y_i, y_j) &= x_i x_j \quad \forall\,i\neq j
\]
\ie the prior distribution for $y$ has been assumed to be a multivariate Gaussian distribution
\[
  y \sim \Normal(0, K)
\]
where $K_{ij} = x_i x_j + \delta_{ij} \sigma_\varepsilon^2$ and $\delta_{ij}$ is the Kronecker delta.
Note that we can write this equation for the covariance as a function of pairs of inputs
\[
K_{ij} = k(x_i, x_j) = x_i x_j + \delta_{x_ix_j} \sigma_\varepsilon^2 \label{eq:intro:lin_noise_k}
\]
and this relationship holds for datasets of arbitrary finite size\footnotemark{}.
We now may reasonably ask whether or not we could define a probability distribution over $y$ when $x$ is infinite \eg when $x = \Reals$.
The answer to this question is yes.

\footnotetext{
We are implicitly assuming that the $x_i$ are disjoint.
This can be relaxed without any conceptual difficulty.
}

\begin{definition}[Gaussian process]
  A Gaussian process (\gp{}) is a collection of random variables, any finite subset of which have a joint Gaussian distribution.
\end{definition}

To specify a Gaussian process we must specify the mean and covariance of any finite subset of random variables which we achieve with a mean function $\mu : \mathcal{X} \to \Reals$ and a covariance function or kernel $k : \mathcal{X}^2 \to \Reals$ where $\mathcal{X}$ is the space inhabited by the inputs $x$ which can be written
\[
  y_i &\sim \mathcal{N}(\mu(x_i), k(x_i, x_i)) \\
  \Cov(y_i, y_j) &= k(x_i, x_j)
\]
or alternatively
\[
  (y_i : i = 1,\dots,n) \sim \Normal\bigg(\big(\mu(x_i) : i = 1,\dots,n\big), K\bigg)
\]
where $K_{ij} = k(x_i, x_j)$; note that $k$ must be such that $K$ is always positive (semi) definite to define a valid probability distribution.
Since these equations hold for any value of the inputs $x$ we can see that by writing $y_i = f(x_i)$ we have essentially specified a probability distribution over functions $f$.
We denote this distribution as
\[
  f \sim \gp(\mu,k).
\]

It is now reasonable to enquire as to the properties of this probability distribution over functions as we change $\mu$ and $k$.
The role of the mean function $\mu$ is particularly easy to characterise; it specifies the pointwise mean of this distribution over functions.
The covariance or kernel $k$ is more interesting.
Random samples from Gaussian processes with a zero mean function but different kernels are shown in figure~\ref{fig:intro:samples}; the randomly sampled functions display qualitatively different patterns of linearity, smoothness and periodicity.

\begin{figure}[ht]
\centering
\includegraphics[width=0.3\columnwidth]{\introfigsdir/lin_prior}
\includegraphics[width=0.3\columnwidth]{\introfigsdir/sq_exp_prior}
\includegraphics[width=0.3\columnwidth]{\introfigsdir/periodic_prior}
\caption[Random Gaussian process samples with different covariance functions.]{
Random samples from zero mean Gaussian processes with different covariance functions.
Lines are random samples from the Gaussian process distributions, the red shading is a representation of the pointwise probability distribution.
\textit{Left}: Linear functions.
\textit{Middle}: Smooth and slowly varying functions.
\textit{Right}: Smooth but exactly periodic functions.
}
\label{fig:intro:samples}
\end{figure}

The distribution over linear functions was constructed using a covariance of the form
\[
  k(x_i, x_j) = x_i x_j
\]
which is the same as that in equation~\eqref{eq:intro:lin_noise_k} with the noise variance equal to zero.

The distribution over smooth functions was constructed with a kernel of the form
\[
  k(x_i, x_j) = \exp(-\alpha|x_i - x_j|^2)
\]
where $\alpha$ is a constant.
When the distance between $x_i$ and $x_j$ is small, the covariance (and correlation) between $f(x_i)$ and $f(x_j)$ will be approximately equal to 1.
As a result, nearby values of the function will be similar to each other.
As the distance $|x_i - x_j|$ increases the correlation monotonically decreases to zero; the exact way in which the correlation decreases determines the characteristic smoothness one observes in figure~\ref{fig:intro:samples}.

Finally, the distribution over periodic functions was constructed using a covariance of the form
\[
  k(x_i, x_j) = \exp\left(-\frac{2\sin^2 (\pi(x_i - x_j)/p)}{\ell^2}\right)
\]
where $\ell$ and $p$ are constants.
Whenever $|x_i - x_j|$ is an integer multiple of $p$ the correlation is equal to 1 resulting in exact periodicity of the random functions.

In chapter~\ref{ch:construction} we demonstrate how to learn an appropriate kernel for a particular dataset by constructing new kernels from old.
In chapter~\ref{ch:description} we demonstrate that by carefully choosing which kernels we compose with each other we can always automatically produce natural language descriptions of the probability distributions they encode for.
In chapter~\ref{ch:gefcom} we demonstrate how these ideas were applied to a data mining competition and where this reveals scope for future work.

\outbpdocument{
\bibliographystyle{plainnat}
\bibliography{references.bib}
}


